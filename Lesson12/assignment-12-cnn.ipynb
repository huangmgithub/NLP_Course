{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from six.moves import cPickle as pickle\n",
    "# from six.moves import range\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/'\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(os.path.join(data_path, pickle_file), 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.572145\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 50: 1.524130\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 100: 1.310477\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 150: 0.440947\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 200: 0.844054\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 250: 1.017027\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 300: 0.749442\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 350: 0.373884\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 400: 1.007475\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 450: 0.643449\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 500: 1.376713\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 550: 0.400898\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 600: 0.513778\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 650: 0.294454\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 700: 0.765589\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 750: 1.133516\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 800: 0.414509\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 850: 0.371341\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 900: 0.775602\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 950: 1.100890\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 1000: 0.698744\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28 # image size\n",
    "num_labels = 10 # labels\n",
    "num_channels = 1 # channel'num\n",
    "num_samples = 200000\n",
    "\n",
    "patch_size = 5 # Filter size\n",
    "depth = 16 # Filter num\n",
    "num_hidden = 64 # Fully Connected Layer's units\n",
    "\n",
    "# Params\n",
    "lam = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "# learning rate（指数衰减法）\n",
    "epochs_per_decay = 2 # 每epochs衰减learning_rate\n",
    "initial_learning_rate = 0.008 # 初始learning_rate\n",
    "learning_rate_decay_factor = 0.95 # 衰减系数\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=([batch_size, image_size, image_size, num_channels]), name=\"images\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=([batch_size, num_labels]), name=\"labels\")\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 16 outputs\n",
    "        'wc1': tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1), name=\"w1\"),\n",
    "        # 5x5 conv, 16 inputs, 16 outputs\n",
    "        'wc2': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1), name=\"w2\"),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1), name=\"w3\"),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name=\"w_out\")\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([depth]), name=\"b1\"),\n",
    "        'bc2': tf.Variable(tf.zeros([depth]), name=\"b2\"),\n",
    "        'bd1': tf.Variable(tf.zeros([num_hidden]), name=\"b3\"),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]), name=\"b_out\")\n",
    "    }\n",
    "\n",
    "    # dropout\n",
    "    keep_pro = tf.placeholder(tf.float32, name='keep_pro')\n",
    "\n",
    "    # Function\n",
    "    def conv2d(x, W, b, strides=1):\n",
    "        # Conv2D wrapper, with bias and relu activation\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def maxpool2d(x, k=2):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME')\n",
    "\n",
    "    def conv_net(x, keep_pro):\n",
    "        # Convolution Layer\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "        # Convolution Layer\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        # Reshape conv2 output to fit fully connected layer input\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.matmul(fc1, weights['wd1']) + biases['bd1']\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # dropout：改善过拟合\n",
    "        fc1 = tf.nn.dropout(fc1, keep_pro)\n",
    "\n",
    "        # Output\n",
    "        out = tf.matmul(fc1, weights['out']) + biases['out']\n",
    "        return out\n",
    "\n",
    "    # output\n",
    "    logits = conv_net(tf_train_data, keep_pro)\n",
    "\n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                          # lam * tf.nn.l2_loss(weights['wc1']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['wc2']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['wd1']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['out'])  # 正则项：改善过拟合\n",
    "                          )\n",
    "\n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # learning rate policy\n",
    "    decay_steps = int(num_samples / batch_size * epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               name='exponential_decay_learning_rate')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    test_output = conv_net(tf_test_data, keep_pro)\n",
    "    test_prediction = tf.nn.softmax(test_output)\n",
    "    \n",
    "    valid_output = conv_net(tf_valid_data, keep_pro)\n",
    "    valid_prediction = tf.nn.softmax(valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, loss:2.3294692039489746\n",
      "Train prediction:16.015625\n",
      "Valid prediction:15.55\n",
      "Test prediction:15.86\n",
      "Step:20, loss:0.6847695112228394\n",
      "Train prediction:80.859375\n",
      "Valid prediction:81.1\n",
      "Test prediction:87.94\n",
      "Step:40, loss:0.5047612190246582\n",
      "Train prediction:86.1328125\n",
      "Valid prediction:84.32\n",
      "Test prediction:90.95\n",
      "Step:60, loss:0.4684596657752991\n",
      "Train prediction:86.328125\n",
      "Valid prediction:85.23\n",
      "Test prediction:91.92\n",
      "Step:80, loss:0.4470149874687195\n",
      "Train prediction:87.5\n",
      "Valid prediction:85.77\n",
      "Test prediction:92.3\n",
      "Step:100, loss:0.5251246094703674\n",
      "Train prediction:86.1328125\n",
      "Valid prediction:86.21\n",
      "Test prediction:92.91\n",
      "Step:120, loss:0.41591376066207886\n",
      "Train prediction:87.5\n",
      "Valid prediction:86.67\n",
      "Test prediction:93.31\n",
      "Step:140, loss:0.46997880935668945\n",
      "Train prediction:85.546875\n",
      "Valid prediction:87.07\n",
      "Test prediction:93.55\n",
      "Step:160, loss:0.4895389974117279\n",
      "Train prediction:84.5703125\n",
      "Valid prediction:87.1\n",
      "Test prediction:93.39\n",
      "Step:180, loss:0.3303981125354767\n",
      "Train prediction:89.84375\n",
      "Valid prediction:87.56\n",
      "Test prediction:93.73\n",
      "Step:200, loss:0.4500904083251953\n",
      "Train prediction:87.109375\n",
      "Valid prediction:88.06\n",
      "Test prediction:94.09\n",
      "Step:220, loss:0.39233535528182983\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:88.45\n",
      "Test prediction:94.34\n",
      "Step:240, loss:0.3993040919303894\n",
      "Train prediction:87.109375\n",
      "Valid prediction:88.56\n",
      "Test prediction:94.41\n",
      "Step:260, loss:0.40912723541259766\n",
      "Train prediction:87.6953125\n",
      "Valid prediction:88.44\n",
      "Test prediction:94.27\n",
      "Step:280, loss:0.3984185755252838\n",
      "Train prediction:87.3046875\n",
      "Valid prediction:88.74\n",
      "Test prediction:94.6\n",
      "Step:300, loss:0.3892112970352173\n",
      "Train prediction:89.0625\n",
      "Valid prediction:88.43\n",
      "Test prediction:94.55\n",
      "Step:320, loss:0.3863495886325836\n",
      "Train prediction:87.5\n",
      "Valid prediction:88.86\n",
      "Test prediction:94.68\n",
      "Step:340, loss:0.34727346897125244\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:88.99\n",
      "Test prediction:94.57\n",
      "Step:360, loss:0.3375203609466553\n",
      "Train prediction:91.015625\n",
      "Valid prediction:88.91\n",
      "Test prediction:94.61\n",
      "Step:380, loss:0.46413809061050415\n",
      "Train prediction:85.9375\n",
      "Valid prediction:89.24\n",
      "Test prediction:94.87\n",
      "Step:0, loss:0.3354445993900299\n",
      "Train prediction:89.453125\n",
      "Valid prediction:88.86\n",
      "Test prediction:94.87\n",
      "Step:20, loss:0.3162057399749756\n",
      "Train prediction:89.84375\n",
      "Valid prediction:89.03\n",
      "Test prediction:94.97\n",
      "Step:40, loss:0.32043421268463135\n",
      "Train prediction:90.625\n",
      "Valid prediction:89.38\n",
      "Test prediction:94.97\n",
      "Step:60, loss:0.3263450264930725\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:89.1\n",
      "Test prediction:94.88\n",
      "Step:80, loss:0.32736843824386597\n",
      "Train prediction:91.015625\n",
      "Valid prediction:89.19\n",
      "Test prediction:94.92\n",
      "Step:100, loss:0.3952716588973999\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:89.84\n",
      "Test prediction:95.25\n",
      "Step:120, loss:0.33982545137405396\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.24\n",
      "Test prediction:94.79\n",
      "Step:140, loss:0.4348788857460022\n",
      "Train prediction:86.71875\n",
      "Valid prediction:89.11\n",
      "Test prediction:94.81\n",
      "Step:160, loss:0.38890695571899414\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:89.51\n",
      "Test prediction:95.27\n",
      "Step:180, loss:0.27212801575660706\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:89.42\n",
      "Test prediction:95.08\n",
      "Step:200, loss:0.3899627923965454\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:89.77\n",
      "Test prediction:95.28\n",
      "Step:220, loss:0.3151928186416626\n",
      "Train prediction:90.625\n",
      "Valid prediction:89.93\n",
      "Test prediction:95.22\n",
      "Step:240, loss:0.30808424949645996\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:89.79\n",
      "Test prediction:95.13\n",
      "Step:260, loss:0.32342541217803955\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.69\n",
      "Test prediction:95.14\n",
      "Step:280, loss:0.37255334854125977\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.04\n",
      "Test prediction:95.3\n",
      "Step:300, loss:0.314374715089798\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.49\n",
      "Test prediction:95.34\n",
      "Step:320, loss:0.3454107642173767\n",
      "Train prediction:89.2578125\n",
      "Valid prediction:89.9\n",
      "Test prediction:95.34\n",
      "Step:340, loss:0.322945773601532\n",
      "Train prediction:88.671875\n",
      "Valid prediction:89.76\n",
      "Test prediction:95.55\n",
      "Step:360, loss:0.32133179903030396\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:89.97\n",
      "Test prediction:95.2\n",
      "Step:380, loss:0.41984325647354126\n",
      "Train prediction:86.1328125\n",
      "Valid prediction:89.89\n",
      "Test prediction:95.56\n",
      "Step:0, loss:0.27280277013778687\n",
      "Train prediction:91.796875\n",
      "Valid prediction:89.74\n",
      "Test prediction:95.23\n",
      "Step:20, loss:0.3146997094154358\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:89.82\n",
      "Test prediction:95.31\n",
      "Step:40, loss:0.2702122926712036\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.25\n",
      "Test prediction:95.47\n",
      "Step:60, loss:0.29150503873825073\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.1\n",
      "Test prediction:95.41\n",
      "Step:80, loss:0.3121494948863983\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.06\n",
      "Test prediction:95.49\n",
      "Step:100, loss:0.37781932950019836\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:89.93\n",
      "Test prediction:95.22\n",
      "Step:120, loss:0.3100121021270752\n",
      "Train prediction:91.40625\n",
      "Valid prediction:89.77\n",
      "Test prediction:95.15\n",
      "Step:140, loss:0.4134909510612488\n",
      "Train prediction:86.5234375\n",
      "Valid prediction:90.06\n",
      "Test prediction:95.36\n",
      "Step:160, loss:0.37034696340560913\n",
      "Train prediction:86.71875\n",
      "Valid prediction:90.17\n",
      "Test prediction:95.64\n",
      "Step:180, loss:0.2130880355834961\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.05\n",
      "Test prediction:95.34\n",
      "Step:200, loss:0.3593854308128357\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.22\n",
      "Test prediction:95.6\n",
      "Step:220, loss:0.3023688793182373\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.68\n",
      "Step:240, loss:0.32255053520202637\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.3\n",
      "Test prediction:95.36\n",
      "Step:260, loss:0.33474230766296387\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.18\n",
      "Test prediction:95.56\n",
      "Step:280, loss:0.3401724696159363\n",
      "Train prediction:88.8671875\n",
      "Valid prediction:89.94\n",
      "Test prediction:95.16\n",
      "Step:300, loss:0.3266041874885559\n",
      "Train prediction:89.84375\n",
      "Valid prediction:89.74\n",
      "Test prediction:95.38\n",
      "Step:320, loss:0.3063524663448334\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.13\n",
      "Test prediction:95.61\n",
      "Step:340, loss:0.30023083090782166\n",
      "Train prediction:89.453125\n",
      "Valid prediction:90.03\n",
      "Test prediction:95.59\n",
      "Step:360, loss:0.29511532187461853\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.28\n",
      "Test prediction:95.79\n",
      "Step:380, loss:0.45971471071243286\n",
      "Train prediction:86.71875\n",
      "Valid prediction:90.12\n",
      "Test prediction:95.53\n",
      "Step:0, loss:0.2774001359939575\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.6\n",
      "Step:20, loss:0.29276058077812195\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.58\n",
      "Step:40, loss:0.2785379886627197\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.27\n",
      "Test prediction:95.5\n",
      "Step:60, loss:0.2993185818195343\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.23\n",
      "Test prediction:95.62\n",
      "Step:80, loss:0.2814500331878662\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.4\n",
      "Test prediction:95.61\n",
      "Step:100, loss:0.37006044387817383\n",
      "Train prediction:89.453125\n",
      "Valid prediction:90.15\n",
      "Test prediction:95.61\n",
      "Step:120, loss:0.2991012632846832\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.59\n",
      "Step:140, loss:0.35701948404312134\n",
      "Train prediction:88.28125\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.54\n",
      "Step:160, loss:0.34169963002204895\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.14\n",
      "Test prediction:95.69\n",
      "Step:180, loss:0.2566581964492798\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.03\n",
      "Test prediction:95.3\n",
      "Step:200, loss:0.3035963177680969\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.28\n",
      "Test prediction:95.57\n",
      "Step:220, loss:0.275549054145813\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.68\n",
      "Step:240, loss:0.2839909791946411\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.53\n",
      "Test prediction:95.72\n",
      "Step:260, loss:0.2783636748790741\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.61\n",
      "Test prediction:95.72\n",
      "Step:280, loss:0.3630422353744507\n",
      "Train prediction:89.84375\n",
      "Valid prediction:90.13\n",
      "Test prediction:95.34\n",
      "Step:300, loss:0.28446143865585327\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.39\n",
      "Test prediction:95.62\n",
      "Step:320, loss:0.29573285579681396\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.26\n",
      "Test prediction:95.81\n",
      "Step:340, loss:0.2975321114063263\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction:95.52\n",
      "Step:360, loss:0.2966621220111847\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.65\n",
      "Step:380, loss:0.4041855037212372\n",
      "Train prediction:87.109375\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.65\n",
      "Step:0, loss:0.27766627073287964\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.41\n",
      "Test prediction:95.7\n",
      "Step:20, loss:0.3266800045967102\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.69\n",
      "Step:40, loss:0.2565458416938782\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.16\n",
      "Test prediction:95.81\n",
      "Step:60, loss:0.293083131313324\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.69\n",
      "Step:80, loss:0.28052887320518494\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.4\n",
      "Test prediction:95.73\n",
      "Step:100, loss:0.32013821601867676\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.87\n",
      "Step:120, loss:0.2666967213153839\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.4\n",
      "Test prediction:95.72\n",
      "Step:140, loss:0.3493283987045288\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.3\n",
      "Test prediction:95.64\n",
      "Step:160, loss:0.3420940041542053\n",
      "Train prediction:89.84375\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.83\n",
      "Step:180, loss:0.2189808338880539\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.24\n",
      "Test prediction:95.59\n",
      "Step:200, loss:0.29999157786369324\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.77\n",
      "Step:220, loss:0.2579920291900635\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.46\n",
      "Test prediction:95.52\n",
      "Step:240, loss:0.3022594749927521\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.87\n",
      "Step:260, loss:0.30118098855018616\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.78\n",
      "Test prediction:95.84\n",
      "Step:280, loss:0.3424127697944641\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:90.38\n",
      "Test prediction:95.53\n",
      "Step:300, loss:0.26624417304992676\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.09\n",
      "Test prediction:95.64\n",
      "Step:320, loss:0.3304489850997925\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.52\n",
      "Test prediction:95.92\n",
      "Step:340, loss:0.27951931953430176\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.48\n",
      "Test prediction:95.85\n",
      "Step:360, loss:0.2727634906768799\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.7\n",
      "Test prediction:95.85\n",
      "Step:380, loss:0.40220150351524353\n",
      "Train prediction:86.9140625\n",
      "Valid prediction:90.68\n",
      "Test prediction:95.95\n",
      "Step:0, loss:0.27335378527641296\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.73\n",
      "Test prediction:95.92\n",
      "Step:20, loss:0.29508817195892334\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.5\n",
      "Test prediction:96.04\n",
      "Step:40, loss:0.23215575516223907\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.59\n",
      "Test prediction:95.84\n",
      "Step:60, loss:0.2821255922317505\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.73\n",
      "Step:80, loss:0.2670440077781677\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.53\n",
      "Test prediction:95.81\n",
      "Step:100, loss:0.33225131034851074\n",
      "Train prediction:89.84375\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.82\n",
      "Step:120, loss:0.24422286450862885\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.41\n",
      "Test prediction:95.8\n",
      "Step:140, loss:0.3270903527736664\n",
      "Train prediction:89.84375\n",
      "Valid prediction:90.51\n",
      "Test prediction:95.82\n",
      "Step:160, loss:0.32571056485176086\n",
      "Train prediction:88.8671875\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.68\n",
      "Step:180, loss:0.20252880454063416\n",
      "Train prediction:93.5546875\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.65\n",
      "Step:200, loss:0.3069040775299072\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.39\n",
      "Test prediction:95.71\n",
      "Step:220, loss:0.26388901472091675\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.74\n",
      "Test prediction:95.85\n",
      "Step:240, loss:0.2509233355522156\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.84\n",
      "Step:260, loss:0.27129319310188293\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.54\n",
      "Test prediction:96.2\n",
      "Step:280, loss:0.28990638256073\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.57\n",
      "Test prediction:95.76\n",
      "Step:300, loss:0.28612232208251953\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.46\n",
      "Test prediction:95.9\n",
      "Step:320, loss:0.3009644150733948\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.74\n",
      "Test prediction:95.85\n",
      "Step:340, loss:0.2855584919452667\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.73\n",
      "Test prediction:95.74\n",
      "Step:360, loss:0.27810198068618774\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.81\n",
      "Step:380, loss:0.3938182592391968\n",
      "Train prediction:86.328125\n",
      "Valid prediction:90.37\n",
      "Test prediction:95.86\n",
      "Step:0, loss:0.26722562313079834\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.69\n",
      "Test prediction:95.84\n",
      "Step:20, loss:0.2869800329208374\n",
      "Train prediction:89.453125\n",
      "Valid prediction:90.58\n",
      "Test prediction:96.01\n",
      "Step:40, loss:0.2608032524585724\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.62\n",
      "Test prediction:95.69\n",
      "Step:60, loss:0.2712266147136688\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.94\n",
      "Step:80, loss:0.26478034257888794\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.9\n",
      "Step:100, loss:0.2897447645664215\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.53\n",
      "Test prediction:95.78\n",
      "Step:120, loss:0.2664027810096741\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.67\n",
      "Test prediction:96.02\n",
      "Step:140, loss:0.33877211809158325\n",
      "Train prediction:89.2578125\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.91\n",
      "Step:160, loss:0.31504154205322266\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.72\n",
      "Test prediction:95.89\n",
      "Step:180, loss:0.22198113799095154\n",
      "Train prediction:93.359375\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.75\n",
      "Step:200, loss:0.3213041126728058\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.8\n",
      "Step:220, loss:0.2554701864719391\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.84\n",
      "Step:240, loss:0.23859745264053345\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.64\n",
      "Test prediction:95.83\n",
      "Step:260, loss:0.2690974771976471\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.85\n",
      "Test prediction:95.99\n",
      "Step:280, loss:0.29229599237442017\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.69\n",
      "Test prediction:95.98\n",
      "Step:300, loss:0.2624291479587555\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.77\n",
      "Test prediction:95.75\n",
      "Step:320, loss:0.30353236198425293\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.74\n",
      "Test prediction:95.81\n",
      "Step:340, loss:0.27526816725730896\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.58\n",
      "Test prediction:96.02\n",
      "Step:360, loss:0.2942468523979187\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.79\n",
      "Test prediction:96.05\n",
      "Step:380, loss:0.403959184885025\n",
      "Train prediction:87.109375\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.73\n",
      "Step:0, loss:0.2807433009147644\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.74\n",
      "Test prediction:95.75\n",
      "Step:20, loss:0.35357892513275146\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.09\n",
      "Test prediction:95.51\n",
      "Step:40, loss:0.24973739683628082\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.42\n",
      "Test prediction:95.59\n",
      "Step:60, loss:0.2626766264438629\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.67\n",
      "Step:80, loss:0.2553648352622986\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.72\n",
      "Test prediction:95.87\n",
      "Step:100, loss:0.28011608123779297\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.61\n",
      "Step:120, loss:0.25659531354904175\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.77\n",
      "Test prediction:95.8\n",
      "Step:140, loss:0.35995572805404663\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.67\n",
      "Test prediction:95.86\n",
      "Step:160, loss:0.3396415710449219\n",
      "Train prediction:89.84375\n",
      "Valid prediction:90.68\n",
      "Test prediction:95.91\n",
      "Step:180, loss:0.20615655183792114\n",
      "Train prediction:93.9453125\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.78\n",
      "Step:200, loss:0.3068154752254486\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.61\n",
      "Test prediction:95.81\n",
      "Step:220, loss:0.24386261403560638\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.93\n",
      "Step:240, loss:0.23980256915092468\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.78\n",
      "Test prediction:96.02\n",
      "Step:260, loss:0.24816671013832092\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.48\n",
      "Test prediction:95.89\n",
      "Step:280, loss:0.28179818391799927\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.82\n",
      "Test prediction:95.99\n",
      "Step:300, loss:0.23867277801036835\n",
      "Train prediction:91.015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid prediction:90.61\n",
      "Test prediction:96.08\n",
      "Step:320, loss:0.27631741762161255\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.93\n",
      "Test prediction:96.08\n",
      "Step:340, loss:0.25257447361946106\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.97\n",
      "Test prediction:95.93\n",
      "Step:360, loss:0.27540576457977295\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.69\n",
      "Test prediction:96.03\n",
      "Step:380, loss:0.41908568143844604\n",
      "Train prediction:86.71875\n",
      "Valid prediction:90.37\n",
      "Test prediction:95.9\n",
      "Step:0, loss:0.25074297189712524\n",
      "Train prediction:93.359375\n",
      "Valid prediction:90.79\n",
      "Test prediction:95.94\n",
      "Step:20, loss:0.25481006503105164\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.83\n",
      "Test prediction:96.03\n",
      "Step:40, loss:0.2709658741950989\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.94\n",
      "Test prediction:95.92\n",
      "Step:60, loss:0.28851017355918884\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.92\n",
      "Step:80, loss:0.2743178606033325\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.7\n",
      "Test prediction:95.77\n",
      "Step:100, loss:0.315923273563385\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.95\n",
      "Step:120, loss:0.2494356334209442\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.89\n",
      "Step:140, loss:0.363804429769516\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.91\n",
      "Step:160, loss:0.2815374732017517\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.47\n",
      "Test prediction:95.67\n",
      "Step:180, loss:0.22135671973228455\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.74\n",
      "Test prediction:95.99\n",
      "Step:200, loss:0.34139835834503174\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:90.72\n",
      "Test prediction:96.04\n",
      "Step:220, loss:0.2581931948661804\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.85\n",
      "Test prediction:95.65\n",
      "Step:240, loss:0.23962080478668213\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.83\n",
      "Test prediction:95.97\n",
      "Step:260, loss:0.24778315424919128\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.74\n",
      "Step:280, loss:0.2897503674030304\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.82\n",
      "Test prediction:96.08\n",
      "Step:300, loss:0.22942498326301575\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.83\n",
      "Test prediction:95.84\n",
      "Step:320, loss:0.28422635793685913\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.8\n",
      "Test prediction:95.98\n",
      "Step:340, loss:0.2552613615989685\n",
      "Train prediction:90.625\n",
      "Valid prediction:91.09\n",
      "Test prediction:95.99\n",
      "Step:360, loss:0.27937430143356323\n",
      "Train prediction:91.015625\n",
      "Valid prediction:91.02\n",
      "Test prediction:95.99\n",
      "Step:380, loss:0.41353076696395874\n",
      "Train prediction:86.9140625\n",
      "Valid prediction:90.55\n",
      "Test prediction:95.97\n",
      "Step:0, loss:0.24811947345733643\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.7\n",
      "Test prediction:96.03\n",
      "Step:20, loss:0.2688763439655304\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.72\n",
      "Test prediction:96.0\n",
      "Step:40, loss:0.22140216827392578\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.79\n",
      "Test prediction:96.02\n",
      "Step:60, loss:0.2720867097377777\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.9\n",
      "Test prediction:96.13\n",
      "Step:80, loss:0.23004919290542603\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.93\n",
      "Test prediction:95.9\n",
      "Step:100, loss:0.22748082876205444\n",
      "Train prediction:93.5546875\n",
      "Valid prediction:90.99\n",
      "Test prediction:95.97\n",
      "Step:120, loss:0.2651406526565552\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.95\n",
      "Step:140, loss:0.36251556873321533\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.53\n",
      "Test prediction:95.88\n",
      "Step:160, loss:0.3272850513458252\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:90.86\n",
      "Test prediction:95.95\n",
      "Step:180, loss:0.2035866379737854\n",
      "Train prediction:93.359375\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.84\n",
      "Step:200, loss:0.3358176648616791\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.5\n",
      "Test prediction:95.95\n",
      "Step:220, loss:0.24182236194610596\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.64\n",
      "Test prediction:96.01\n",
      "Step:240, loss:0.26164543628692627\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.4\n",
      "Test prediction:95.59\n",
      "Step:260, loss:0.2785787582397461\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.72\n",
      "Test prediction:95.89\n",
      "Step:280, loss:0.2902197241783142\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.69\n",
      "Test prediction:95.75\n",
      "Step:300, loss:0.25650811195373535\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.98\n",
      "Step:320, loss:0.27898192405700684\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.64\n",
      "Test prediction:95.94\n",
      "Step:340, loss:0.2844197750091553\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.93\n",
      "Step:360, loss:0.2856929898262024\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.89\n",
      "Test prediction:96.15\n",
      "Step:380, loss:0.3721560835838318\n",
      "Train prediction:88.671875\n",
      "Valid prediction:90.73\n",
      "Test prediction:96.3\n"
     ]
    }
   ],
   "source": [
    "# save loss\n",
    "loss_array = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = int(num_samples / batch_size)\n",
    "        for step in range(total_batch):\n",
    "            start = step * batch_size\n",
    "            end = (step + 1) * batch_size\n",
    "            batch_data, batch_labels = train_dataset[start:end,:,:,:], train_labels[start:end,:]\n",
    "\n",
    "            # run session\n",
    "            _, batch_loss, prediction = sess.run([optimizer, loss, train_prediction],\n",
    "                                                 feed_dict={tf_train_data: batch_data,\n",
    "                                                            tf_train_labels: batch_labels,\n",
    "                                                            keep_pro: 0.75})\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                print(\"Step:{}, loss:{}\".format(step, batch_loss))\n",
    "                print(\"Train prediction:{}\".format(accuracy(prediction, batch_labels)))\n",
    "                print(\"Valid prediction:{}\".format(accuracy(valid_prediction.eval(\n",
    "                    feed_dict={tf_train_data: batch_data, tf_train_labels: batch_labels, keep_pro: 1.0}\n",
    "                ), valid_labels)))\n",
    "                print(\"Test prediction:{}\".format(accuracy(test_prediction.eval(\n",
    "                    feed_dict={tf_train_data: batch_data, tf_train_labels: batch_labels, keep_pro: 1.0}\n",
    "                ), test_labels)))\n",
    "\n",
    "        loss_array.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 11:04:42.709559  6412 legend.py:1289] No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2630bae95c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.array(range(n_epochs)), np.array(loss_array))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28 # image size\n",
    "num_labels = 10 # labels\n",
    "num_channels = 1 # channel'num\n",
    "num_samples = 200000\n",
    "\n",
    "patch_size = 5 # Filter size\n",
    "depth_1 = 6 # Filter num\n",
    "depth_2 = 16\n",
    "depth_3 = 120\n",
    "num_hidden = 84 # Fully Connected Layer's units\n",
    "\n",
    "# Params\n",
    "lam = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "# learning rate（指数衰减法）\n",
    "epochs_per_decay = 2 # 每epochs衰减learning_rate\n",
    "initial_learning_rate = 0.008 # 初始learning_rate\n",
    "learning_rate_decay_factor = 0.95 # 衰减系数\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=([batch_size, image_size, image_size, num_channels]), name=\"images\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=([batch_size, num_labels]), name=\"labels\")\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 6 outputs\n",
    "        'wc1': tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1), name=\"w1\"),\n",
    "        # 5x5 conv, 6 inputs, 16 outputs\n",
    "        'wc2': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1), name=\"w2\"),\n",
    "         # 5x5 conv, 16 inputs, 120 outputs\n",
    "        'wc3': tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1), name=\"w3\"),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.truncated_normal([1 * 1 * depth_3, num_hidden], stddev=0.1), name=\"wd\"),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1), name=\"w_out\")\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros([depth_1]), name=\"b1\"),\n",
    "        'bc2': tf.Variable(tf.zeros([depth_2]), name=\"b2\"),\n",
    "        'bc3': tf.Variable(tf.zeros([depth_3]), name=\"b3\"),\n",
    "        'bd1': tf.Variable(tf.zeros([num_hidden]), name=\"bd\"),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]), name=\"b_out\")\n",
    "    }\n",
    "\n",
    "    # dropout\n",
    "    keep_pro = tf.placeholder(tf.float32, name='keep_pro')\n",
    "\n",
    "    # Function\n",
    "    def conv2d(x, W, b, strides=1, padding='SAME'):\n",
    "        # Conv2D wrapper, with bias and relu activation\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def maxpool2d(x, k=2, padding='SAME'):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding=padding)\n",
    "\n",
    "    def conv_net(x, keep_pro):\n",
    "        # Convolution Layer 1\n",
    "        conv1 = conv2d(x, weights['wc1'], biases['bc1'], padding='SAME')\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1 = maxpool2d(conv1, k=2, padding='SAME')\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        conv2 = conv2d(conv1, weights['wc2'], biases['bc2'], padding='VALID')\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2 = maxpool2d(conv2, k=2, padding='SAME')\n",
    "        \n",
    "        # Convolution Layer 3\n",
    "        conv3 = conv2d(conv2, weights['wc3'], biases['bc3'], padding='VALID')\n",
    "\n",
    "        # Fully Connected layer\n",
    "        # Reshape conv2 output to fit fully connected layer input\n",
    "        fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.matmul(fc1, weights['wd1']) + biases['bd1']\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # dropout：改善过拟合\n",
    "        fc1 = tf.nn.dropout(fc1, keep_pro)\n",
    "\n",
    "        # Output\n",
    "        out = tf.matmul(fc1, weights['out']) + biases['out']\n",
    "        return out\n",
    "\n",
    "    # output\n",
    "    logits = conv_net(tf_train_data, keep_pro)\n",
    "\n",
    "    # Loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                          # lam * tf.nn.l2_loss(weights['wc1']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['wc2']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['wd1']) +\n",
    "                          # lam * tf.nn.l2_loss(weights['out'])  # 正则项：改善过拟合\n",
    "                          )\n",
    "\n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # learning rate policy\n",
    "    decay_steps = int(num_samples / batch_size * epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               name='exponential_decay_learning_rate')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Prediction\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    test_output = conv_net(tf_test_data, keep_pro)\n",
    "    test_prediction = tf.nn.softmax(test_output)\n",
    "    \n",
    "    valid_output = conv_net(tf_valid_data, keep_pro)\n",
    "    valid_prediction = tf.nn.softmax(valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0, loss:2.314842700958252\n",
      "Train prediction:11.9140625\n",
      "Valid prediction:23.94\n",
      "Test prediction:25.95\n",
      "Step:20, loss:0.7306785583496094\n",
      "Train prediction:77.9296875\n",
      "Valid prediction:79.54\n",
      "Test prediction:86.28\n",
      "Step:40, loss:0.4639003574848175\n",
      "Train prediction:87.3046875\n",
      "Valid prediction:83.32\n",
      "Test prediction:90.11\n",
      "Step:60, loss:0.46597668528556824\n",
      "Train prediction:86.9140625\n",
      "Valid prediction:84.53\n",
      "Test prediction:91.17\n",
      "Step:80, loss:0.4338417649269104\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:85.56\n",
      "Test prediction:91.84\n",
      "Step:100, loss:0.4984356462955475\n",
      "Train prediction:87.109375\n",
      "Valid prediction:85.93\n",
      "Test prediction:92.31\n",
      "Step:120, loss:0.40820416808128357\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:87.35\n",
      "Test prediction:93.16\n",
      "Step:140, loss:0.4719412326812744\n",
      "Train prediction:84.765625\n",
      "Valid prediction:87.14\n",
      "Test prediction:93.15\n",
      "Step:160, loss:0.4141617715358734\n",
      "Train prediction:88.28125\n",
      "Valid prediction:87.42\n",
      "Test prediction:93.68\n",
      "Step:180, loss:0.3282180726528168\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:87.38\n",
      "Test prediction:93.69\n",
      "Step:200, loss:0.4448202848434448\n",
      "Train prediction:87.6953125\n",
      "Valid prediction:87.58\n",
      "Test prediction:93.71\n",
      "Step:220, loss:0.3772774338722229\n",
      "Train prediction:89.0625\n",
      "Valid prediction:87.77\n",
      "Test prediction:93.92\n",
      "Step:240, loss:0.36537396907806396\n",
      "Train prediction:88.8671875\n",
      "Valid prediction:88.14\n",
      "Test prediction:94.25\n",
      "Step:260, loss:0.37342602014541626\n",
      "Train prediction:88.8671875\n",
      "Valid prediction:88.39\n",
      "Test prediction:94.21\n",
      "Step:280, loss:0.4068351984024048\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:88.15\n",
      "Test prediction:94.11\n",
      "Step:300, loss:0.3556155562400818\n",
      "Train prediction:88.28125\n",
      "Valid prediction:88.04\n",
      "Test prediction:93.9\n",
      "Step:320, loss:0.3701414465904236\n",
      "Train prediction:88.671875\n",
      "Valid prediction:88.85\n",
      "Test prediction:94.73\n",
      "Step:340, loss:0.34919553995132446\n",
      "Train prediction:89.453125\n",
      "Valid prediction:88.64\n",
      "Test prediction:94.34\n",
      "Step:360, loss:0.31393545866012573\n",
      "Train prediction:90.625\n",
      "Valid prediction:88.75\n",
      "Test prediction:94.52\n",
      "Step:380, loss:0.4771221876144409\n",
      "Train prediction:84.9609375\n",
      "Valid prediction:88.77\n",
      "Test prediction:94.61\n",
      "Step:0, loss:0.2972397208213806\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:88.44\n",
      "Test prediction:94.31\n",
      "Step:20, loss:0.3059973120689392\n",
      "Train prediction:91.40625\n",
      "Valid prediction:88.94\n",
      "Test prediction:94.81\n",
      "Step:40, loss:0.285211443901062\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:88.91\n",
      "Test prediction:94.52\n",
      "Step:60, loss:0.3144127428531647\n",
      "Train prediction:90.234375\n",
      "Valid prediction:89.37\n",
      "Test prediction:94.8\n",
      "Step:80, loss:0.3511257767677307\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:88.75\n",
      "Test prediction:94.43\n",
      "Step:100, loss:0.33080220222473145\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:88.78\n",
      "Test prediction:94.64\n",
      "Step:120, loss:0.32999151945114136\n",
      "Train prediction:91.015625\n",
      "Valid prediction:89.47\n",
      "Test prediction:95.01\n",
      "Step:140, loss:0.39641591906547546\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:88.67\n",
      "Test prediction:94.49\n",
      "Step:160, loss:0.3496391177177429\n",
      "Train prediction:88.28125\n",
      "Valid prediction:89.39\n",
      "Test prediction:94.97\n",
      "Step:180, loss:0.25088047981262207\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:89.09\n",
      "Test prediction:94.97\n",
      "Step:200, loss:0.3955037295818329\n",
      "Train prediction:89.453125\n",
      "Valid prediction:89.52\n",
      "Test prediction:94.93\n",
      "Step:220, loss:0.3113307058811188\n",
      "Train prediction:89.84375\n",
      "Valid prediction:89.44\n",
      "Test prediction:95.1\n",
      "Step:240, loss:0.32464703917503357\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:89.78\n",
      "Test prediction:95.16\n",
      "Step:260, loss:0.311014324426651\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.89\n",
      "Test prediction:95.15\n",
      "Step:280, loss:0.3122546672821045\n",
      "Train prediction:89.84375\n",
      "Valid prediction:89.96\n",
      "Test prediction:95.28\n",
      "Step:300, loss:0.301699161529541\n",
      "Train prediction:90.625\n",
      "Valid prediction:89.85\n",
      "Test prediction:95.35\n",
      "Step:320, loss:0.3483312427997589\n",
      "Train prediction:89.453125\n",
      "Valid prediction:89.81\n",
      "Test prediction:95.5\n",
      "Step:340, loss:0.3046056926250458\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:89.8\n",
      "Test prediction:95.29\n",
      "Step:360, loss:0.2956680655479431\n",
      "Train prediction:91.015625\n",
      "Valid prediction:89.72\n",
      "Test prediction:95.1\n",
      "Step:380, loss:0.44715654850006104\n",
      "Train prediction:85.546875\n",
      "Valid prediction:89.72\n",
      "Test prediction:95.27\n",
      "Step:0, loss:0.29589933156967163\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.3\n",
      "Test prediction:94.76\n",
      "Step:20, loss:0.2765379846096039\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:89.78\n",
      "Test prediction:95.32\n",
      "Step:40, loss:0.25650840997695923\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.17\n",
      "Test prediction:95.51\n",
      "Step:60, loss:0.26660001277923584\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:89.88\n",
      "Test prediction:95.28\n",
      "Step:80, loss:0.28634700179100037\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.05\n",
      "Test prediction:95.33\n",
      "Step:100, loss:0.3492715060710907\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:89.96\n",
      "Test prediction:95.2\n",
      "Step:120, loss:0.2997523248195648\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.05\n",
      "Test prediction:95.32\n",
      "Step:140, loss:0.3734474182128906\n",
      "Train prediction:89.0625\n",
      "Valid prediction:89.92\n",
      "Test prediction:95.51\n",
      "Step:160, loss:0.3319704532623291\n",
      "Train prediction:89.84375\n",
      "Valid prediction:89.87\n",
      "Test prediction:95.42\n",
      "Step:180, loss:0.22323504090309143\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:89.73\n",
      "Test prediction:95.25\n",
      "Step:200, loss:0.33297115564346313\n",
      "Train prediction:91.40625\n",
      "Valid prediction:89.91\n",
      "Test prediction:95.52\n",
      "Step:220, loss:0.2778092622756958\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:89.82\n",
      "Test prediction:95.12\n",
      "Step:240, loss:0.27967771887779236\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.19\n",
      "Test prediction:95.58\n",
      "Step:260, loss:0.2672656178474426\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.39\n",
      "Test prediction:95.68\n",
      "Step:280, loss:0.3122273087501526\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.07\n",
      "Test prediction:95.55\n",
      "Step:300, loss:0.2871851921081543\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.16\n",
      "Test prediction:95.5\n",
      "Step:320, loss:0.3309963047504425\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:90.26\n",
      "Test prediction:95.42\n",
      "Step:340, loss:0.3056495189666748\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.24\n",
      "Test prediction:95.53\n",
      "Step:360, loss:0.2762508988380432\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:89.93\n",
      "Test prediction:95.48\n",
      "Step:380, loss:0.4196389317512512\n",
      "Train prediction:87.3046875\n",
      "Valid prediction:90.18\n",
      "Test prediction:95.27\n",
      "Step:0, loss:0.27743279933929443\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.02\n",
      "Test prediction:95.28\n",
      "Step:20, loss:0.24810074269771576\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.04\n",
      "Test prediction:95.41\n",
      "Step:40, loss:0.24947813153266907\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.4\n",
      "Test prediction:95.58\n",
      "Step:60, loss:0.24670207500457764\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.2\n",
      "Test prediction:95.58\n",
      "Step:80, loss:0.3002524673938751\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.28\n",
      "Test prediction:95.37\n",
      "Step:100, loss:0.3175591230392456\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:89.93\n",
      "Test prediction:95.43\n",
      "Step:120, loss:0.26968854665756226\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.47\n",
      "Step:140, loss:0.3652515411376953\n",
      "Train prediction:89.0625\n",
      "Valid prediction:89.81\n",
      "Test prediction:95.32\n",
      "Step:160, loss:0.30482882261276245\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.09\n",
      "Test prediction:95.57\n",
      "Step:180, loss:0.21834048628807068\n",
      "Train prediction:94.7265625\n",
      "Valid prediction:90.1\n",
      "Test prediction:95.21\n",
      "Step:200, loss:0.30804598331451416\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.41\n",
      "Test prediction:95.76\n",
      "Step:220, loss:0.2593039572238922\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.25\n",
      "Test prediction:95.43\n",
      "Step:240, loss:0.28262704610824585\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.28\n",
      "Test prediction:95.65\n",
      "Step:260, loss:0.25109729170799255\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.77\n",
      "Step:280, loss:0.28227120637893677\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.2\n",
      "Test prediction:95.57\n",
      "Step:300, loss:0.26000428199768066\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.11\n",
      "Test prediction:95.52\n",
      "Step:320, loss:0.29833123087882996\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.61\n",
      "Step:340, loss:0.27768105268478394\n",
      "Train prediction:90.4296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid prediction:90.37\n",
      "Test prediction:95.5\n",
      "Step:360, loss:0.2329525649547577\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.3\n",
      "Test prediction:95.7\n",
      "Step:380, loss:0.41190463304519653\n",
      "Train prediction:86.71875\n",
      "Valid prediction:90.42\n",
      "Test prediction:95.56\n",
      "Step:0, loss:0.2540034353733063\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.2\n",
      "Test prediction:95.35\n",
      "Step:20, loss:0.27186983823776245\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.17\n",
      "Test prediction:95.48\n",
      "Step:40, loss:0.23917952179908752\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.7\n",
      "Test prediction:95.84\n",
      "Step:60, loss:0.2530757188796997\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.5\n",
      "Test prediction:95.65\n",
      "Step:80, loss:0.25656992197036743\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.58\n",
      "Test prediction:95.56\n",
      "Step:100, loss:0.32097581028938293\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.07\n",
      "Test prediction:95.33\n",
      "Step:120, loss:0.2595524787902832\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.29\n",
      "Test prediction:95.5\n",
      "Step:140, loss:0.3527764081954956\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:89.79\n",
      "Test prediction:95.41\n",
      "Step:160, loss:0.30897924304008484\n",
      "Train prediction:90.0390625\n",
      "Valid prediction:90.25\n",
      "Test prediction:95.63\n",
      "Step:180, loss:0.19814974069595337\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.19\n",
      "Test prediction:95.64\n",
      "Step:200, loss:0.2934648096561432\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.52\n",
      "Test prediction:95.97\n",
      "Step:220, loss:0.28112518787384033\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.31\n",
      "Test prediction:95.48\n",
      "Step:240, loss:0.26461130380630493\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.68\n",
      "Step:260, loss:0.27706223726272583\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.79\n",
      "Step:280, loss:0.310625284910202\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.34\n",
      "Test prediction:95.72\n",
      "Step:300, loss:0.24979086220264435\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.35\n",
      "Test prediction:95.63\n",
      "Step:320, loss:0.30719518661499023\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:90.58\n",
      "Test prediction:95.71\n",
      "Step:340, loss:0.26121246814727783\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.18\n",
      "Test prediction:95.45\n",
      "Step:360, loss:0.2298816740512848\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.19\n",
      "Test prediction:95.47\n",
      "Step:380, loss:0.4095454216003418\n",
      "Train prediction:87.890625\n",
      "Valid prediction:90.2\n",
      "Test prediction:95.47\n",
      "Step:0, loss:0.24256069958209991\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.61\n",
      "Step:20, loss:0.24405735731124878\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.38\n",
      "Test prediction:95.74\n",
      "Step:40, loss:0.21830734610557556\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.62\n",
      "Test prediction:95.82\n",
      "Step:60, loss:0.2180914580821991\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.7\n",
      "Step:80, loss:0.25502628087997437\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.67\n",
      "Step:100, loss:0.30835676193237305\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.26\n",
      "Test prediction:95.48\n",
      "Step:120, loss:0.2511465847492218\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.67\n",
      "Step:140, loss:0.3273083567619324\n",
      "Train prediction:88.8671875\n",
      "Valid prediction:89.95\n",
      "Test prediction:95.53\n",
      "Step:160, loss:0.33086177706718445\n",
      "Train prediction:89.453125\n",
      "Valid prediction:90.39\n",
      "Test prediction:95.66\n",
      "Step:180, loss:0.20022669434547424\n",
      "Train prediction:94.7265625\n",
      "Valid prediction:90.36\n",
      "Test prediction:95.66\n",
      "Step:200, loss:0.31060248613357544\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.36\n",
      "Test prediction:95.59\n",
      "Step:220, loss:0.27448171377182007\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.37\n",
      "Step:240, loss:0.23900029063224792\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.57\n",
      "Test prediction:95.67\n",
      "Step:260, loss:0.2596820592880249\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.52\n",
      "Test prediction:95.57\n",
      "Step:280, loss:0.284458190202713\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.83\n",
      "Test prediction:95.76\n",
      "Step:300, loss:0.250855952501297\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.59\n",
      "Step:320, loss:0.27169275283813477\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.64\n",
      "Test prediction:95.75\n",
      "Step:340, loss:0.31108614802360535\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.19\n",
      "Test prediction:95.47\n",
      "Step:360, loss:0.2372244894504547\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.35\n",
      "Test prediction:95.55\n",
      "Step:380, loss:0.37748005986213684\n",
      "Train prediction:88.0859375\n",
      "Valid prediction:90.03\n",
      "Test prediction:95.51\n",
      "Step:0, loss:0.258187472820282\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.48\n",
      "Test prediction:95.7\n",
      "Step:20, loss:0.2637845277786255\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.71\n",
      "Step:40, loss:0.20158106088638306\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.87\n",
      "Step:60, loss:0.22238200902938843\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.31\n",
      "Test prediction:95.61\n",
      "Step:80, loss:0.23910623788833618\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.61\n",
      "Test prediction:95.8\n",
      "Step:100, loss:0.30634182691574097\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.5\n",
      "Test prediction:95.44\n",
      "Step:120, loss:0.23974335193634033\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.69\n",
      "Test prediction:95.62\n",
      "Step:140, loss:0.33381640911102295\n",
      "Train prediction:89.453125\n",
      "Valid prediction:90.24\n",
      "Test prediction:95.53\n",
      "Step:160, loss:0.3097950220108032\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.17\n",
      "Test prediction:95.63\n",
      "Step:180, loss:0.2108561396598816\n",
      "Train prediction:93.9453125\n",
      "Valid prediction:90.29\n",
      "Test prediction:95.64\n",
      "Step:200, loss:0.302417516708374\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.82\n",
      "Step:220, loss:0.2725781798362732\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.86\n",
      "Test prediction:95.61\n",
      "Step:240, loss:0.26056838035583496\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.63\n",
      "Test prediction:95.74\n",
      "Step:260, loss:0.2099764049053192\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.89\n",
      "Step:280, loss:0.3214225769042969\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.57\n",
      "Test prediction:95.77\n",
      "Step:300, loss:0.25446707010269165\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.49\n",
      "Test prediction:95.55\n",
      "Step:320, loss:0.26086023449897766\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.58\n",
      "Test prediction:95.7\n",
      "Step:340, loss:0.2567647099494934\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.67\n",
      "Test prediction:95.61\n",
      "Step:360, loss:0.23146884143352509\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.62\n",
      "Test prediction:95.69\n",
      "Step:380, loss:0.3993568420410156\n",
      "Train prediction:88.4765625\n",
      "Valid prediction:90.63\n",
      "Test prediction:95.81\n",
      "Step:0, loss:0.2608441710472107\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.48\n",
      "Step:20, loss:0.24620862305164337\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.45\n",
      "Test prediction:95.63\n",
      "Step:40, loss:0.2344064712524414\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.73\n",
      "Test prediction:95.7\n",
      "Step:60, loss:0.22344529628753662\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.35\n",
      "Test prediction:95.48\n",
      "Step:80, loss:0.2501748204231262\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.51\n",
      "Test prediction:95.6\n",
      "Step:100, loss:0.2904413342475891\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.28\n",
      "Test prediction:95.64\n",
      "Step:120, loss:0.21668648719787598\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.47\n",
      "Test prediction:95.58\n",
      "Step:140, loss:0.30594587326049805\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.0\n",
      "Test prediction:95.24\n",
      "Step:160, loss:0.31321558356285095\n",
      "Train prediction:89.6484375\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.64\n",
      "Step:180, loss:0.2211017608642578\n",
      "Train prediction:93.5546875\n",
      "Valid prediction:90.56\n",
      "Test prediction:95.64\n",
      "Step:200, loss:0.29825547337532043\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.62\n",
      "Test prediction:95.82\n",
      "Step:220, loss:0.24193920195102692\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.39\n",
      "Test prediction:95.53\n",
      "Step:240, loss:0.2830878496170044\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.61\n",
      "Step:260, loss:0.203684002161026\n",
      "Train prediction:93.359375\n",
      "Valid prediction:90.38\n",
      "Test prediction:95.78\n",
      "Step:280, loss:0.29233524203300476\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.69\n",
      "Test prediction:95.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:300, loss:0.24132849276065826\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.81\n",
      "Test prediction:95.81\n",
      "Step:320, loss:0.2652839720249176\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.67\n",
      "Test prediction:95.89\n",
      "Step:340, loss:0.2637340724468231\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.55\n",
      "Test prediction:95.78\n",
      "Step:360, loss:0.2112874537706375\n",
      "Train prediction:93.1640625\n",
      "Valid prediction:90.25\n",
      "Test prediction:95.74\n",
      "Step:380, loss:0.35433194041252136\n",
      "Train prediction:89.2578125\n",
      "Valid prediction:90.6\n",
      "Test prediction:95.82\n",
      "Step:0, loss:0.24671533703804016\n",
      "Train prediction:91.2109375\n",
      "Valid prediction:90.82\n",
      "Test prediction:95.78\n",
      "Step:20, loss:0.24279999732971191\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.69\n",
      "Step:40, loss:0.1881858855485916\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.96\n",
      "Test prediction:96.02\n",
      "Step:60, loss:0.21876323223114014\n",
      "Train prediction:93.5546875\n",
      "Valid prediction:90.83\n",
      "Test prediction:95.83\n",
      "Step:80, loss:0.2764732241630554\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.99\n",
      "Step:100, loss:0.26102566719055176\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.82\n",
      "Step:120, loss:0.20941707491874695\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.82\n",
      "Test prediction:95.82\n",
      "Step:140, loss:0.3435022532939911\n",
      "Train prediction:88.671875\n",
      "Valid prediction:90.41\n",
      "Test prediction:95.53\n",
      "Step:160, loss:0.2688741385936737\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.73\n",
      "Test prediction:95.8\n",
      "Step:180, loss:0.2029995173215866\n",
      "Train prediction:94.53125\n",
      "Valid prediction:90.59\n",
      "Test prediction:95.76\n",
      "Step:200, loss:0.27629899978637695\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:91.02\n",
      "Test prediction:95.91\n",
      "Step:220, loss:0.23969891667366028\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.7\n",
      "Test prediction:95.85\n",
      "Step:240, loss:0.23771941661834717\n",
      "Train prediction:91.796875\n",
      "Valid prediction:90.76\n",
      "Test prediction:95.83\n",
      "Step:260, loss:0.2293109893798828\n",
      "Train prediction:93.75\n",
      "Valid prediction:90.52\n",
      "Test prediction:95.75\n",
      "Step:280, loss:0.2897823750972748\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.65\n",
      "Step:300, loss:0.2533040940761566\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.29\n",
      "Test prediction:95.5\n",
      "Step:320, loss:0.2604455053806305\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.61\n",
      "Test prediction:95.83\n",
      "Step:340, loss:0.23062147200107574\n",
      "Train prediction:92.578125\n",
      "Valid prediction:90.35\n",
      "Test prediction:95.72\n",
      "Step:360, loss:0.2125551700592041\n",
      "Train prediction:92.1875\n",
      "Valid prediction:90.61\n",
      "Test prediction:95.82\n",
      "Step:380, loss:0.3521377742290497\n",
      "Train prediction:89.0625\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.8\n",
      "Step:0, loss:0.2452007234096527\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.59\n",
      "Test prediction:95.72\n",
      "Step:20, loss:0.2654589116573334\n",
      "Train prediction:91.015625\n",
      "Valid prediction:90.12\n",
      "Test prediction:95.45\n",
      "Step:40, loss:0.22717797756195068\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.78\n",
      "Test prediction:95.61\n",
      "Step:60, loss:0.20351004600524902\n",
      "Train prediction:92.96875\n",
      "Valid prediction:90.62\n",
      "Test prediction:95.46\n",
      "Step:80, loss:0.2691580057144165\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.83\n",
      "Test prediction:95.7\n",
      "Step:100, loss:0.24218443036079407\n",
      "Train prediction:91.6015625\n",
      "Valid prediction:90.71\n",
      "Test prediction:95.62\n",
      "Step:120, loss:0.20821323990821838\n",
      "Train prediction:94.140625\n",
      "Valid prediction:90.58\n",
      "Test prediction:95.6\n",
      "Step:140, loss:0.2872174382209778\n",
      "Train prediction:90.4296875\n",
      "Valid prediction:90.59\n",
      "Test prediction:95.7\n",
      "Step:160, loss:0.27831971645355225\n",
      "Train prediction:90.625\n",
      "Valid prediction:90.47\n",
      "Test prediction:95.79\n",
      "Step:180, loss:0.19022464752197266\n",
      "Train prediction:94.3359375\n",
      "Valid prediction:90.82\n",
      "Test prediction:95.8\n",
      "Step:200, loss:0.30635830760002136\n",
      "Train prediction:91.9921875\n",
      "Valid prediction:90.87\n",
      "Test prediction:95.88\n",
      "Step:220, loss:0.2333812564611435\n",
      "Train prediction:92.3828125\n",
      "Valid prediction:90.65\n",
      "Test prediction:95.68\n",
      "Step:240, loss:0.23693616688251495\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.81\n",
      "Test prediction:95.88\n",
      "Step:260, loss:0.25191783905029297\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.66\n",
      "Test prediction:95.64\n",
      "Step:280, loss:0.283883273601532\n",
      "Train prediction:90.234375\n",
      "Valid prediction:90.33\n",
      "Test prediction:95.47\n",
      "Step:300, loss:0.24115388095378876\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.53\n",
      "Test prediction:95.56\n",
      "Step:320, loss:0.26688137650489807\n",
      "Train prediction:91.40625\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.96\n",
      "Step:340, loss:0.28621765971183777\n",
      "Train prediction:90.8203125\n",
      "Valid prediction:90.44\n",
      "Test prediction:95.83\n",
      "Step:360, loss:0.2360144853591919\n",
      "Train prediction:92.7734375\n",
      "Valid prediction:90.56\n",
      "Test prediction:96.09\n",
      "Step:380, loss:0.35790058970451355\n",
      "Train prediction:89.2578125\n",
      "Valid prediction:90.54\n",
      "Test prediction:95.82\n"
     ]
    }
   ],
   "source": [
    "# save loss\n",
    "loss_array = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = int(num_samples / batch_size)\n",
    "        for step in range(total_batch):\n",
    "            start = step * batch_size\n",
    "            end = (step + 1) * batch_size\n",
    "            batch_data, batch_labels = train_dataset[start:end,:,:,:], train_labels[start:end,:]\n",
    "\n",
    "            # run session\n",
    "            _, batch_loss, prediction = sess.run([optimizer, loss, train_prediction],\n",
    "                                                 feed_dict={tf_train_data: batch_data,\n",
    "                                                            tf_train_labels: batch_labels,\n",
    "                                                            keep_pro: 0.75})\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                print(\"Step:{}, loss:{}\".format(step, batch_loss))\n",
    "                print(\"Train prediction:{}\".format(accuracy(prediction, batch_labels)))\n",
    "                print(\"Valid prediction:{}\".format(accuracy(valid_prediction.eval(\n",
    "                    feed_dict={tf_train_data: batch_data, tf_train_labels: batch_labels, keep_pro: 1.0}\n",
    "                ), valid_labels)))\n",
    "                print(\"Test prediction:{}\".format(accuracy(test_prediction.eval(\n",
    "                    feed_dict={tf_train_data: batch_data, tf_train_labels: batch_labels, keep_pro: 1.0}\n",
    "                ), test_labels)))\n",
    "\n",
    "        loss_array.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 11:49:12.805558  6412 legend.py:1289] No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26311e394e0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.array(range(n_epochs)), np.array(loss_array))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
