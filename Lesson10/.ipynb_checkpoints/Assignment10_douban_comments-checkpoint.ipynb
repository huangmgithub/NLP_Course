{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/huangm/Desktop/NLP/movie_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤缺失值\n",
    "data.dropna(subset=['comment', 'star'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['吴京意淫到了脑残的地步，看了恶心想吐', '1'],\n",
       "       ['首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹',\n",
       "        '2'],\n",
       "       ['吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。',\n",
       "        '2'],\n",
       "       ...,\n",
       "       ['我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）', 2],\n",
       "       ['冲着颜值还可以看下去', 3],\n",
       "       ['除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心', 3]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看star值的种类\n",
    "data[[\"comment\",\"star\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    83808\n",
       "3    65672\n",
       "5    59315\n",
       "2    28136\n",
       "1    24564\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替代star\n",
    "data['star'].replace(\"star\",1, inplace=True)\n",
    "data['star'].replace(\"1\", 1, inplace=True)\n",
    "data['star'].replace(\"2\",2, inplace=True)\n",
    "data['star'].replace(\"3\",3, inplace=True)\n",
    "data['star'].replace(\"4\",4, inplace=True)\n",
    "data['star'].replace(\"5\",5, inplace=True)\n",
    "data.star.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    4\n",
       "4    1\n",
       "5    1\n",
       "6    2\n",
       "7    4\n",
       "8    4\n",
       "9    1\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.star[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(['id','link','name', 'star'], axis=1)\n",
    "# y = data.drop(['id','link','name','comment'], axis=1)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, list, 261495, list)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = data.comment.tolist()\n",
    "stars = data.star.tolist()\n",
    "len(comments), type(comments), len(stars), type(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return ' '.join(jieba.cut(string))\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"匹配文字\"\"\"\n",
    "    return re.findall(r'[\\d\\w]+', string)\n",
    "\n",
    "# with open('../../data/comments.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for comment in comments:\n",
    "#         contents = cut(''.join(token(comment)))\n",
    "#         f.write(contents + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [cut(''.join(token(comment))) for comment in comments]\n",
    "stars = [int(star) for star in stars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # sklearn tfidf向量化\n",
    "\n",
    "vectorized = TfidfVectorizer(ngram_range=(1, 3), max_features=200)\n",
    "comments_vectors = vectorized.fit_transform(comments)\n",
    "comments_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_array = comments_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167356, 200), 167356, (41840, 200), 41840, (52299, 200), 52299)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(comments_array, stars, test_size=0.2, random_state=52)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=32)\n",
    "x_train.shape, len(y_train), x_valid.shape, len(y_valid), x_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167356, 5) (41840, 5) (52299, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(labels):\n",
    "    \"\"\"Reformat label style\"\"\"\n",
    "    labels = (np.arange(1, 6) == labels[:,None]).astype(np.float32) \n",
    "    return labels\n",
    "\n",
    "y_train = reformat(np.array(y_train))\n",
    "y_valid = reformat(np.array(y_valid))\n",
    "y_test = reformat(np.array(y_test))\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32) \n",
    "x_test = x_test.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"compute accuracy\"\"\"\n",
    "    return 100 * np.sum(np.argmax(y_pred, 1) == np.argmax(y_true, 1)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden1_units = 500 # Number of first hidden layer units\n",
    "hidden2_units = 400 # Number of second hidden layer units\n",
    "\n",
    "num_classes = 5\n",
    "batch_size = 10000    \n",
    "num_epochs = 20\n",
    "\n",
    "initial_learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.95\n",
    "num_epochs_per_decay = 100\n",
    "\n",
    "lam = 0.002\n",
    "\n",
    "# Dimension of train sets\n",
    "d = x_train.shape\n",
    "n_train_samples = d[0] # the num of train sets\n",
    "n_features = d[1]  # features num\n",
    "\n",
    "# compute graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_features), name=\"comment\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_classes), name='star')\n",
    "    tf_valid_dataset = tf.constant(x_valid)\n",
    "    tf_test_dataset = tf.constant(x_test)\n",
    "    \n",
    "    # 1 layer\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([n_features, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(n_features))), name=\"weights1\")\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]), name=\"biases1\")\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    # 2 layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))), name=\"weights2\")\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_units]), name=\"biases2\")\n",
    "    \n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    \n",
    "    # 3 layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, num_classes],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden2_units))), name=\"weights3\")\n",
    "    biases3 = tf.Variable(tf.zeros([num_classes]), name=\"biases3\")\n",
    "    \n",
    "    # 增加dropout, 改善过拟合\n",
    "    keep_pro = tf.placeholder(tf.float32)\n",
    "    hidden2_drop = tf.nn.dropout(hidden2, keep_pro)\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden2_drop, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) + \n",
    "        lam * tf.nn.l2_loss(weights1) +\n",
    "        lam * tf.nn.l2_loss(weights2) +\n",
    "        lam * tf.nn.l2_loss(weights3)) # 增加 正则项\n",
    "     \n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # learning rate policy\n",
    "    decay_steps = int(n_train_samples / batch_size * num_epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               name=\"exponential_decay_learning_rate\")\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden_valid_2 = tf.nn.relu(tf.matmul(hidden_valid_1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_2, weights3) + biases3)\n",
    "    \n",
    "    # test\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.191146\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 5: 2.084550\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 10: 1.993348\n",
      "Train accuracy: 32.1%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.4%\n",
      "Loss at step 15: 1.910497\n",
      "Train accuracy: 32.9%\n",
      "Validation accuracy: 33.0%\n",
      "Test accuracy: 32.9%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    # 初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch_training = int(n_train_samples / batch_size) # train 次数 per epoch\n",
    "\n",
    "        for batch_num in range(total_batch_training):\n",
    "            start = batch_num * batch_size\n",
    "            end = (batch_num + 1) * batch_size\n",
    "\n",
    "            # Fit training using batch data\n",
    "            train_batch_data, train_batch_labels = x_train[start:end], y_train[start:end]\n",
    "            \n",
    "            # run session\n",
    "            batch_loss, _, predictions = sess.run(\n",
    "                [loss, optimizer, train_prediction],\n",
    "            feed_dict={tf_train_dataset:train_batch_data, tf_train_labels:train_batch_labels, keep_pro:0.5})\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (epoch, batch_loss))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, train_batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_valid)) # 1\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "                test_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_test)) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 150000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = data.comment.tolist()[:150000]\n",
    "stars = data.star.tolist()[:150000]\n",
    "len(comments), len(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import jieba\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.findall(r'[\\d\\w]+', string)\n",
    "    string = ''.join(string)\n",
    "    return string.strip()\n",
    "\n",
    "def process_labels(labels):\n",
    "    \"\"\"Reformat label style\"\"\"\n",
    "    labels = np.array(labels)\n",
    "    labels = (np.arange(1, 6) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Process data\n",
    "    data_sets = [clean_str(sent) for sent in data]\n",
    "    data_sets = [cut(s) for s in data_sets]\n",
    "    return data_sets\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data(data,labels):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences = process_data(data)\n",
    "    labels = process_labels(labels)\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\huangm\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.695 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((150000, 1881), (150000, 5), 117743)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(comments, stars)\n",
    "data[0].shape, data[1].shape, len(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "class TextCNN:\n",
    "    def __init__(self, sequence_length, n_classes, vocab_size,\n",
    "                 embedding_size, filter_sizes, n_filters, l2_reg_lambda):\n",
    "        \"\"\"\n",
    "        A CNN for text classification.\n",
    "        Uses an embedding layer, followed by a convolution, max-pooling and softmax layer.\n",
    "\n",
    "        :param sequence_length: 序列长度\n",
    "        :param n_classes: 类别数量\n",
    "        :param vocab_size: 词库大小\n",
    "        :param embedding_size: 嵌入大小\n",
    "        :param filter_sizes: 过滤器大小（width不变，height变化）的集合\n",
    "        :param n_filters: 相同大小的过滤器的数量\n",
    "        :param l2_reg_lambda: L2 正则化参数\n",
    "        :param input_x: batch data\n",
    "        :param input_y: batch labels\n",
    "        :param dropout_keep_pro: dropout参数\n",
    "        \"\"\"\n",
    "        \n",
    "        # Placeholder for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, shape=(None, sequence_length), name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=(None, n_classes), name=\"input_y\")\n",
    "        self.dropout_keep_pro = tf.placeholder(tf.float32, name=\"dropout_keep_pro\")\n",
    "\n",
    "        # Keepping track of l2 regularization los\n",
    "        l2_loss = tf.constant(0.)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            # W包含词库大小的词嵌入，随机初始化\n",
    "            self.W = tf.Variable(tf.random_normal([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
    "            # 输入序列词对应的嵌入，(None, sequence_length, embedding_size)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            # 增加一个维度, 扩展至四维 (None, sequence_length, embedding_size, 1)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + max_pool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # W: [filter_height, filter_width, in_channels, out_channels]\n",
    "                # Input: [batch, in_height, in_width, in_channels]\n",
    "                # b: [out_channels]\n",
    "                filter_shape = [filter_size, embedding_size, 1, n_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[n_filters]), name='b')\n",
    "                conv = tf.nn.conv2d(self.embedded_chars_expanded,\n",
    "                                    W,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\",\n",
    "                                    name=\"conv\")\n",
    "                # Apply non linearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Max pooling over the outputs\n",
    "                # height after conv：sequence_length - filter_size + 1\n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding=\"VALID\",\n",
    "                                        name=\"pool\")\n",
    "                # pooled_outputs: [batch, height, width, channels]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        n_filters_total = n_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, n_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_pro)\n",
    "\n",
    "        # Final (unnormalized) scores and prediction\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[n_filters_total, n_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer()\n",
    "            )\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            # l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"logits\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(loss + l2_reg_lambda * l2_loss) # Add L2 Loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "def train(x_train, y_train, x_test, y_test, vocab_size):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        sequence_length = x_train.shape[1]\n",
    "        n_classes = y_train.shape[1]\n",
    "\n",
    "        cnn = TextCNN(\n",
    "        sequence_length = sequence_length,\n",
    "        n_classes = n_classes,\n",
    "        vocab_size = vocab_size,\n",
    "        embedding_size = 300,\n",
    "        filter_sizes = [2,3,4],\n",
    "        n_filters = 32,\n",
    "        l2_reg_lambda = 0.001)\n",
    "\n",
    "        # Define Training procedure\n",
    "        loss = cnn.loss\n",
    "        accuracy = cnn.accuracy\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_batches = x_train.shape[0] // batch_size\n",
    "            for step in range(total_batches):\n",
    "                x_batch = x_train[step * batch_size:(step+1) * batch_size,:]\n",
    "                y_batch = y_train[step * batch_size:(step+1) * batch_size,:]\n",
    "                feed_dict = {\n",
    "                            cnn.input_x: x_batch,\n",
    "                            cnn.input_y: y_batch,\n",
    "                            cnn.dropout_keep_pro: 0.5\n",
    "                            }\n",
    "                batch_loss, batch_accuracy = sess.run(\n",
    "                            [loss, accuracy], feed_dict)\n",
    "                time_str = datetime.datetime.now()\n",
    "                if step % 10 == 0:\n",
    "                    print(\"{}: step {}, loss {}, acc {}\".format(time_str, step, batch_loss, batch_accuracy))\n",
    "        # Test\n",
    "        x_batch_test = x_test[:1024]\n",
    "        y_batch_test = y_test[:1024]\n",
    "        feed_dict = {\n",
    "                    cnn.input_x: x_batch_test,\n",
    "                    cnn.input_y: y_batch_test,\n",
    "                    cnn.dropout_keep_pro: 1\n",
    "                    }\n",
    "        batch_loss, batch_accuracy = sess.run(\n",
    "                            [loss, accuracy], feed_dict)\n",
    "        time_str = datetime.datetime.now()\n",
    "        print(\"{}: step {}, loss {}, acc {}\".format(time_str, step, batch_loss, batch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1001 00:00:21.653234  7920 deprecation.py:506] From <ipython-input-14-3bbcbc360ff5>:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1001 00:00:26.963678  7920 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1001 00:00:26.995398  7920 deprecation.py:323] From <ipython-input-14-3bbcbc360ff5>:96: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-01 00:01:19.024293: step 0, loss 11.828598022460938, acc 0.1171875\n",
      "2019-10-01 00:08:00.076423: step 10, loss 11.002870559692383, acc 0.21875\n",
      "2019-10-01 00:13:58.637581: step 20, loss 8.990839004516602, acc 0.28125\n",
      "2019-10-01 00:19:50.584288: step 30, loss 12.057561874389648, acc 0.21875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "vocab_size = len(data[2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "train(X_train, y_train, X_test, y_test, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  star\n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐     1\n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...     2\n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...     2\n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。     4\n",
       "4                                               中二得很     1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(['id','link','name'], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196121, 2), (65374, 2))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, random_state=20)\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import fasttext\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"匹配文字\"\"\"\n",
    "    return re.findall(r'[\\d\\w]+', string)\n",
    "\n",
    "def get_stopwords(filename):\n",
    "    \"\"\"停用词\"\"\"\n",
    "    stopwords = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip())\n",
    "    return set(stopwords)\n",
    "        \n",
    "def preprocess_text(comments, stopwords):\n",
    "    \"\"\"预处理\"\"\"\n",
    "    comment_list = []\n",
    "    for comment, star in comments.values:\n",
    "        words = cut(''.join(token(comment))) # 分词 and 正则\n",
    "        words = filter(lambda x:len(x)>1, words)\n",
    "        words = filter(lambda x:x not in stopwords, words)\n",
    "        comment_list.append('__label__' + str(star) + ' ' + ''.join(words))\n",
    "    return comment_list\n",
    "\n",
    "def save_to_file(comment_list, filename):\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f:\n",
    "        for comment in comment_list:\n",
    "            f.write(comment + '\\n')\n",
    "\n",
    "def preprocess_data(comments, stopwords, save_filename):\n",
    "    \n",
    "    # 预处理\n",
    "    comment_list = preprocess_text(comments, stopwords)\n",
    "    \n",
    "    # 训练数据\n",
    "    save_to_file(comment_list, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196121, 0.945115515421602, 0.945115515421602),\n",
       " (65374, 0.409566494324961, 0.409566494324961))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_file_path = './chinese_stopwords.txt'\n",
    "save_file_path_1 = '../../data/train_data.txt'\n",
    "save_file_path_2 = '../../data/test_data.txt'\n",
    "\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "\n",
    "preprocess_data(train_data, stopwords, save_file_path_1)\n",
    "preprocess_data(test_data, stopwords, save_file_path_2)\n",
    "\n",
    "# Fasttext监督学习\n",
    "ft = fasttext.train_supervised(save_file_path_1, lr=1e-2, epoch=20,  wordNgrams=2)\n",
    "\n",
    "ft.test(save_file_path_1),ft.test(save_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.predict('放假了算法思路放好了')[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['4星半', 5],\n",
       "       ['我个人非常不喜欢这片子，也许是不喜欢近几年这类型的所有片子：一股阴湿腻味的腥气扑面而来。里头许多角色貌似都是得过精神病的甲亢患者，跟孙海英入过一个教（真有个教主唉）。有些武侠有侠气，有些有鬼气，有些是仙气，而有些只剩下血腥气和肉膈气。纯靠真爱的面孔和川普撑到最后。',\n",
       "        2],\n",
       "       ['后半段没有共鸣', 4],\n",
       "       ['导演是不是也想某天被意外撞死啊？？？？？？', 3],\n",
       "       ['歌好听', 4]], dtype=object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196121,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_list = []\n",
    "for x, _ in train_data.values:\n",
    "    x = ''.join(cut(''.join(token(x))))\n",
    "    y_pred = ft.predict(x)[0][0][-1]\n",
    "    train_pred_list.append(int(y_pred))\n",
    "train_pred = np.array(train_pred_list)\n",
    "train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196121,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true= train_data['star'].values\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32184722696702545"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65374,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_list = []\n",
    "for x, _ in test_data.values:\n",
    "    x = ''.join(cut(''.join(token(x))))\n",
    "    y_pred = ft.predict(x)[0][0][-1]\n",
    "    test_pred_list.append(int(y_pred))\n",
    "test_pred = np.array(test_pred_list)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65374,)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true= test_data['star'].values\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26951081469697435"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
