{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/huangm/Desktop/movie_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤缺失值\n",
    "data.dropna(subset=['comment', 'star'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       43002\n",
       "4       40806\n",
       "3       33910\n",
       "5       31947\n",
       "3       31762\n",
       "5       27368\n",
       "2       14299\n",
       "2       13837\n",
       "1       12308\n",
       "1       12255\n",
       "star        1\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看star值的种类\n",
    "data.star.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    43002\n",
       "4    40806\n",
       "3    33910\n",
       "5    31947\n",
       "3    31762\n",
       "5    27368\n",
       "2    14299\n",
       "2    13837\n",
       "1    12308\n",
       "1    12256\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替代star\n",
    "data['star'].replace([\"star\"],[\"1\"], inplace=True )\n",
    "data.star.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(['id','link','name', 'star'], axis=1)\n",
    "# y = data.drop(['id','link','name','comment'], axis=1)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, list, 261495, list)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = data.comment.tolist()\n",
    "stars = data.star.tolist()\n",
    "len(comments), type(comments), len(stars), type(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0911 21:15:21.046540  2628 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\huangm\\AppData\\Local\\Temp\\jieba.cache\n",
      "I0911 21:15:21.047543  2628 __init__.py:131] Loading model from cache C:\\Users\\huangm\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.597 seconds.\n",
      "I0911 21:15:21.644128  2628 __init__.py:163] Loading model cost 0.597 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "I0911 21:15:21.645132  2628 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return ' '.join(jieba.cut(string))\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"匹配文字\"\"\"\n",
    "    return re.findall(r'[\\d\\w]+', string)\n",
    "\n",
    "with open('./comments.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for comment in comments:\n",
    "        contents = cut(''.join(token(comment)))\n",
    "        f.write(contents + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [cut(''.join(token(comment))) for comment in comments]\n",
    "stars = [int(star) for star in stars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # sklearn tfidf向量化\n",
    "\n",
    "vectorized = TfidfVectorizer(ngram_range=(1, 3), max_features=300)\n",
    "comments_vectors = vectorized.fit_transform(comments)\n",
    "comments_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_array = comments_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167356, 300), 167356, (41840, 300), 41840, (52299, 300), 52299)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(comments_array, stars, test_size=0.2, random_state=52)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=32)\n",
    "x_train.shape, len(y_train), x_valid.shape, len(y_valid), x_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167356, 5) (41840, 5) (52299, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(labels):\n",
    "    \"\"\"Reformat label style\"\"\"\n",
    "    labels = (np.arange(1, 6) == labels[:,None]).astype(np.float32) \n",
    "    return labels\n",
    "\n",
    "y_train = reformat(np.array(y_train))\n",
    "y_valid = reformat(np.array(y_valid))\n",
    "y_test = reformat(np.array(y_test))\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32) \n",
    "x_test = x_test.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"compute accuracy\"\"\"\n",
    "    return 100 * np.sum(np.argmax(y_pred, 1) == np.argmax(y_true, 1)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden1_units = 500 # Number of first hidden layer units\n",
    "hidden2_units = 400 # Number of second hidden layer units\n",
    "\n",
    "num_classes = 5\n",
    "batch_size = 10000    \n",
    "num_epochs = 200\n",
    "\n",
    "initial_learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.95\n",
    "num_epochs_per_decay = 100\n",
    "\n",
    "lam = 0.002\n",
    "\n",
    "# Dimension of train sets\n",
    "d = x_train.shape\n",
    "n_train_samples = d[0] # the num of train sets\n",
    "n_features = d[1]  # features num\n",
    "\n",
    "# compute graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_features), name=\"comment\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_classes), name='star')\n",
    "    tf_valid_dataset = tf.constant(x_valid)\n",
    "    tf_test_dataset = tf.constant(x_test)\n",
    "    \n",
    "    # 1 layer\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([n_features, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(n_features))), name=\"weights1\")\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]), name=\"biases1\")\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    # 2 layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))), name=\"weights2\")\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_units]), name=\"biases2\")\n",
    "    \n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    \n",
    "    # 3 layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, num_classes],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden2_units))), name=\"weights3\")\n",
    "    biases3 = tf.Variable(tf.zeros([num_classes]), name=\"biases3\")\n",
    "    \n",
    "    # 增加dropout, 改善过拟合\n",
    "    keep_pro = tf.placeholder(tf.float32)\n",
    "    hidden2_drop = tf.nn.dropout(hidden2, keep_pro)\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden2_drop, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) + \n",
    "        lam * tf.nn.l2_loss(weights1) +\n",
    "        lam * tf.nn.l2_loss(weights2) +\n",
    "        lam * tf.nn.l2_loss(weights3)) # 增加 正则项\n",
    "     \n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # learning rate policy\n",
    "    decay_steps = int(n_train_samples / batch_size * num_epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               name=\"exponential_decay_learning_rate\")\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden_valid_2 = tf.nn.relu(tf.matmul(hidden_valid_1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_2, weights3) + biases3)\n",
    "    \n",
    "    # test\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.194586\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 5: 2.089600\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 10: 2.000487\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 15: 1.922598\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 20: 1.848259\n",
      "Train accuracy: 32.5%\n",
      "Validation accuracy: 32.9%\n",
      "Test accuracy: 32.8%\n",
      "Loss at step 25: 1.774283\n",
      "Train accuracy: 33.5%\n",
      "Validation accuracy: 34.3%\n",
      "Test accuracy: 34.2%\n",
      "Loss at step 30: 1.733163\n",
      "Train accuracy: 33.4%\n",
      "Validation accuracy: 34.9%\n",
      "Test accuracy: 34.6%\n",
      "Loss at step 35: 1.684019\n",
      "Train accuracy: 35.1%\n",
      "Validation accuracy: 35.9%\n",
      "Test accuracy: 35.6%\n",
      "Loss at step 40: 1.647416\n",
      "Train accuracy: 35.6%\n",
      "Validation accuracy: 36.3%\n",
      "Test accuracy: 35.9%\n",
      "Loss at step 45: 1.615625\n",
      "Train accuracy: 36.0%\n",
      "Validation accuracy: 36.4%\n",
      "Test accuracy: 36.1%\n",
      "Loss at step 50: 1.589488\n",
      "Train accuracy: 36.1%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.2%\n",
      "Loss at step 55: 1.567568\n",
      "Train accuracy: 35.9%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.3%\n",
      "Loss at step 60: 1.549591\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 65: 1.532537\n",
      "Train accuracy: 36.5%\n",
      "Validation accuracy: 37.2%\n",
      "Test accuracy: 36.9%\n",
      "Loss at step 70: 1.528423\n",
      "Train accuracy: 35.6%\n",
      "Validation accuracy: 36.3%\n",
      "Test accuracy: 36.0%\n",
      "Loss at step 75: 1.514676\n",
      "Train accuracy: 36.3%\n",
      "Validation accuracy: 36.9%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 80: 1.501395\n",
      "Train accuracy: 35.9%\n",
      "Validation accuracy: 36.8%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 85: 1.495798\n",
      "Train accuracy: 36.4%\n",
      "Validation accuracy: 37.2%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 90: 1.495700\n",
      "Train accuracy: 35.4%\n",
      "Validation accuracy: 36.2%\n",
      "Test accuracy: 35.9%\n",
      "Loss at step 95: 1.483800\n",
      "Train accuracy: 36.8%\n",
      "Validation accuracy: 37.5%\n",
      "Test accuracy: 37.1%\n",
      "Loss at step 100: 1.482021\n",
      "Train accuracy: 36.1%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 105: 1.469116\n",
      "Train accuracy: 36.4%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.8%\n",
      "Loss at step 110: 1.469156\n",
      "Train accuracy: 36.3%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 115: 1.468078\n",
      "Train accuracy: 36.5%\n",
      "Validation accuracy: 37.2%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 120: 1.463861\n",
      "Train accuracy: 36.5%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.6%\n",
      "Loss at step 125: 1.465529\n",
      "Train accuracy: 35.8%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 130: 1.461611\n",
      "Train accuracy: 35.7%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 135: 1.459439\n",
      "Train accuracy: 36.3%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.5%\n",
      "Loss at step 140: 1.457394\n",
      "Train accuracy: 35.9%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 145: 1.458556\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 36.7%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 150: 1.454258\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 37.0%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 155: 1.454022\n",
      "Train accuracy: 36.1%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.6%\n",
      "Loss at step 160: 1.458155\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.6%\n",
      "Loss at step 165: 1.452534\n",
      "Train accuracy: 36.5%\n",
      "Validation accuracy: 37.2%\n",
      "Test accuracy: 36.8%\n",
      "Loss at step 170: 1.450830\n",
      "Train accuracy: 36.4%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 175: 1.454369\n",
      "Train accuracy: 36.0%\n",
      "Validation accuracy: 36.6%\n",
      "Test accuracy: 36.4%\n",
      "Loss at step 180: 1.450495\n",
      "Train accuracy: 36.1%\n",
      "Validation accuracy: 37.0%\n",
      "Test accuracy: 36.7%\n",
      "Loss at step 185: 1.451216\n",
      "Train accuracy: 35.8%\n",
      "Validation accuracy: 36.8%\n",
      "Test accuracy: 36.5%\n",
      "Loss at step 190: 1.450284\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.6%\n",
      "Loss at step 195: 1.451059\n",
      "Train accuracy: 36.2%\n",
      "Validation accuracy: 37.1%\n",
      "Test accuracy: 36.6%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    # 初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch_training = int(n_train_samples / batch_size) # train 次数 per epoch\n",
    "\n",
    "        for batch_num in range(total_batch_training):\n",
    "            start = batch_num * batch_size\n",
    "            end = (batch_num + 1) * batch_size\n",
    "\n",
    "            # Fit training using batch data\n",
    "            train_batch_data, train_batch_labels = x_train[start:end], y_train[start:end]\n",
    "            \n",
    "            # run session\n",
    "            batch_loss, _, predictions = sess.run(\n",
    "                [loss, optimizer, train_prediction],\n",
    "            feed_dict={tf_train_dataset:train_batch_data, tf_train_labels:train_batch_labels, keep_pro:0.5})\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (epoch, batch_loss))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, train_batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_valid)) # 1\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "                test_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_test)) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models.word2vec import LineSentence #文件读取\n",
    "# import multiprocessing #多进程\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(lineno)d -  %(message)s')\n",
    "\n",
    "# def word2vec_train():\n",
    "#     \"\"\"词向量训练\"\"\"\n",
    "#     f_wiki = open(\"./wiki.txt\", \"r\", encoding=\"utf-8\")\n",
    "#     sentences = LineSentence(f_wiki)\n",
    "#     model = Word2Vec(sentences, size = 200, window = 10, min_count = 500, workers = multiprocessing.cpu_count()) # 可视化快一些\n",
    "#     model.save(\"./wiki.zh.model\")\n",
    "#     model.wv.save_word2vec_format(\"./wiki.zh.vectors\", binary = False)\n",
    "    \n",
    "# word2vec_model = Word2Vec.load(\"./wiki.zh.model\")\n",
    "# #查看词向量\n",
    "# print(\"江西\", word2vec_model['江西']) # 获得单个单词的词向量\n",
    "# print(word2vec_model.similarity(\"江西\",\"福建\")) # 比较两个单词的相似度\n",
    "# sim_words = word2vec_model.most_similar('江西') # 获得近义词\n",
    "# for w in sim_words:\n",
    "#     print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 看样子效果很差啊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
