{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/huangm/Desktop/movie_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤缺失值\n",
    "data.dropna(subset=['comment', 'star'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       43002\n",
       "4       40806\n",
       "3       33910\n",
       "5       31947\n",
       "3       31762\n",
       "5       27368\n",
       "2       14299\n",
       "2       13837\n",
       "1       12308\n",
       "1       12255\n",
       "star        1\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看star值的种类\n",
    "data.star.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    83808\n",
       "3    65672\n",
       "5    59315\n",
       "2    28136\n",
       "1    24564\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替代star\n",
    "data['star'].replace(\"star\",1, inplace=True)\n",
    "data['star'].replace(\"1\", 1, inplace=True)\n",
    "data['star'].replace(\"2\",2, inplace=True)\n",
    "data['star'].replace(\"3\",3, inplace=True)\n",
    "data['star'].replace(\"4\",4, inplace=True)\n",
    "data['star'].replace(\"5\",5, inplace=True)\n",
    "data.star.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "3    4\n",
       "4    1\n",
       "5    1\n",
       "6    2\n",
       "7    4\n",
       "8    4\n",
       "9    1\n",
       "Name: star, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.star[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(['id','link','name', 'star'], axis=1)\n",
    "# y = data.drop(['id','link','name','comment'], axis=1)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, list, 261495, list)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = data.comment.tolist()\n",
    "stars = data.star.tolist()\n",
    "len(comments), type(comments), len(stars), type(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return ' '.join(jieba.cut(string))\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"匹配文字\"\"\"\n",
    "    return re.findall(r'[\\d\\w]+', string)\n",
    "\n",
    "# with open('../../data/comments.txt', 'w', encoding=\"utf-8\") as f:\n",
    "#     for comment in comments:\n",
    "#         contents = cut(''.join(token(comment)))\n",
    "#         f.write(contents + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [cut(''.join(token(comment))) for comment in comments]\n",
    "stars = [int(star) for star in stars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 200)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # sklearn tfidf向量化\n",
    "\n",
    "vectorized = TfidfVectorizer(ngram_range=(1, 3), max_features=200)\n",
    "comments_vectors = vectorized.fit_transform(comments)\n",
    "comments_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_array = comments_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167356, 200), 167356, (41840, 200), 41840, (52299, 200), 52299)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(comments_array, stars, test_size=0.2, random_state=52)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=32)\n",
    "x_train.shape, len(y_train), x_valid.shape, len(y_valid), x_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167356, 5) (41840, 5) (52299, 5)\n"
     ]
    }
   ],
   "source": [
    "def reformat(labels):\n",
    "    \"\"\"Reformat label style\"\"\"\n",
    "    labels = (np.arange(1, 6) == labels[:,None]).astype(np.float32) \n",
    "    return labels\n",
    "\n",
    "y_train = reformat(np.array(y_train))\n",
    "y_valid = reformat(np.array(y_valid))\n",
    "y_test = reformat(np.array(y_test))\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32) \n",
    "x_test = x_test.astype(np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"compute accuracy\"\"\"\n",
    "    return 100 * np.sum(np.argmax(y_pred, 1) == np.argmax(y_true, 1)) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden1_units = 500 # Number of first hidden layer units\n",
    "hidden2_units = 400 # Number of second hidden layer units\n",
    "\n",
    "num_classes = 5\n",
    "batch_size = 10000    \n",
    "num_epochs = 20\n",
    "\n",
    "initial_learning_rate = 0.5\n",
    "learning_rate_decay_factor = 0.95\n",
    "num_epochs_per_decay = 100\n",
    "\n",
    "lam = 0.002\n",
    "\n",
    "# Dimension of train sets\n",
    "d = x_train.shape\n",
    "n_train_samples = d[0] # the num of train sets\n",
    "n_features = d[1]  # features num\n",
    "\n",
    "# compute graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, n_features), name=\"comment\")\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_classes), name='star')\n",
    "    tf_valid_dataset = tf.constant(x_valid)\n",
    "    tf_test_dataset = tf.constant(x_test)\n",
    "    \n",
    "    # 1 layer\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([n_features, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(n_features))), name=\"weights1\")\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]), name=\"biases1\")\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    # 2 layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))), name=\"weights2\")\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_units]), name=\"biases2\")\n",
    "    \n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    \n",
    "    # 3 layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, num_classes],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden2_units))), name=\"weights3\")\n",
    "    biases3 = tf.Variable(tf.zeros([num_classes]), name=\"biases3\")\n",
    "    \n",
    "    # 增加dropout, 改善过拟合\n",
    "    keep_pro = tf.placeholder(tf.float32)\n",
    "    hidden2_drop = tf.nn.dropout(hidden2, keep_pro)\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden2_drop, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) + \n",
    "        lam * tf.nn.l2_loss(weights1) +\n",
    "        lam * tf.nn.l2_loss(weights2) +\n",
    "        lam * tf.nn.l2_loss(weights3)) # 增加 正则项\n",
    "     \n",
    "    # global step\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # learning rate policy\n",
    "    decay_steps = int(n_train_samples / batch_size * num_epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               name=\"exponential_decay_learning_rate\")\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden_valid_2 = tf.nn.relu(tf.matmul(hidden_valid_1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_2, weights3) + biases3)\n",
    "    \n",
    "    # test\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.191806\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 5: 2.085783\n",
      "Train accuracy: 32.0%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 10: 1.994474\n",
      "Train accuracy: 32.1%\n",
      "Validation accuracy: 32.3%\n",
      "Test accuracy: 32.3%\n",
      "Loss at step 15: 1.909983\n",
      "Train accuracy: 32.9%\n",
      "Validation accuracy: 33.0%\n",
      "Test accuracy: 33.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    # 初始化\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch_training = int(n_train_samples / batch_size) # train 次数 per epoch\n",
    "\n",
    "        for batch_num in range(total_batch_training):\n",
    "            start = batch_num * batch_size\n",
    "            end = (batch_num + 1) * batch_size\n",
    "\n",
    "            # Fit training using batch data\n",
    "            train_batch_data, train_batch_labels = x_train[start:end], y_train[start:end]\n",
    "            \n",
    "            # run session\n",
    "            batch_loss, _, predictions = sess.run(\n",
    "                [loss, optimizer, train_prediction],\n",
    "            feed_dict={tf_train_dataset:train_batch_data, tf_train_labels:train_batch_labels, keep_pro:0.5})\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (epoch, batch_loss))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, train_batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_valid)) # 1\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "                test_prediction.eval({tf_train_dataset: train_batch_data, tf_train_labels:train_batch_labels, keep_pro:1}), y_test)) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  star\n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐     1\n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...     2\n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...     2\n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。     4\n",
       "4                                               中二得很     1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(['id','link','name'], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196121, 2), (65374, 2))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, random_state=20)\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import fasttext\n",
    "\n",
    "def cut(string):\n",
    "    \"\"\"切词\"\"\"\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "def token(string):\n",
    "    \"\"\"匹配文字\"\"\"\n",
    "    return re.findall(r'[\\d\\w]+', string)\n",
    "\n",
    "def get_stopwords(filename):\n",
    "    \"\"\"停用词\"\"\"\n",
    "    stopwords = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip())\n",
    "    return set(stopwords)\n",
    "        \n",
    "def preprocess_text(comments, stopwords):\n",
    "    \"\"\"预处理\"\"\"\n",
    "    comment_list = []\n",
    "    for comment, star in comments.values:\n",
    "        words = cut(''.join(token(comment))) # 分词 and 正则\n",
    "        words = filter(lambda x:len(x)>1, words)\n",
    "        words = filter(lambda x:x not in stopwords, words)\n",
    "        comment_list.append('__label__' + str(star) + ' ' + ''.join(words))\n",
    "    return comment_list\n",
    "\n",
    "def save_to_file(comment_list, filename):\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f:\n",
    "        for comment in comment_list:\n",
    "            f.write(comment + '\\n')\n",
    "\n",
    "def preprocess_data(comments, stopwords, save_filename):\n",
    "    \n",
    "    # 预处理\n",
    "    comment_list = preprocess_text(comments, stopwords)\n",
    "    \n",
    "    # 训练数据\n",
    "    save_to_file(comment_list, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196121, 0.945115515421602, 0.945115515421602),\n",
       " (65374, 0.409566494324961, 0.409566494324961))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_file_path = './chinese_stopwords.txt'\n",
    "save_file_path_1 = '../../data/train_data.txt'\n",
    "save_file_path_2 = '../../data/test_data.txt'\n",
    "\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "\n",
    "preprocess_data(train_data, stopwords, save_file_path_1)\n",
    "preprocess_data(test_data, stopwords, save_file_path_2)\n",
    "\n",
    "# Fasttext监督学习\n",
    "ft = fasttext.train_supervised(save_file_path_1, lr=1e-2, epoch=20,  wordNgrams=2)\n",
    "\n",
    "ft.test(save_file_path_1),ft.test(save_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.predict('放假了算法思路放好了')[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['4星半', 5],\n",
       "       ['我个人非常不喜欢这片子，也许是不喜欢近几年这类型的所有片子：一股阴湿腻味的腥气扑面而来。里头许多角色貌似都是得过精神病的甲亢患者，跟孙海英入过一个教（真有个教主唉）。有些武侠有侠气，有些有鬼气，有些是仙气，而有些只剩下血腥气和肉膈气。纯靠真爱的面孔和川普撑到最后。',\n",
       "        2],\n",
       "       ['后半段没有共鸣', 4],\n",
       "       ['导演是不是也想某天被意外撞死啊？？？？？？', 3],\n",
       "       ['歌好听', 4]], dtype=object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196121,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_list = []\n",
    "for x, _ in train_data.values:\n",
    "    x = ''.join(cut(''.join(token(x))))\n",
    "    y_pred = ft.predict(x)[0][0][-1]\n",
    "    train_pred_list.append(int(y_pred))\n",
    "train_pred = np.array(train_pred_list)\n",
    "train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196121,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true= train_data['star'].values\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32184722696702545"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65374,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_list = []\n",
    "for x, _ in test_data.values:\n",
    "    x = ''.join(cut(''.join(token(x))))\n",
    "    y_pred = ft.predict(x)[0][0][-1]\n",
    "    test_pred_list.append(int(y_pred))\n",
    "test_pred = np.array(test_pred_list)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65374,)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true= test_data['star'].values\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26951081469697435"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_pred == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
