{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in 1_notmnist.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_set (192407, 28, 28) (192407,)\n",
      "Validation_set (10000, 28, 28) (10000,)\n",
      "Test_set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../../data/notMNIST_sanitized.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset_sanitized']\n",
    "    train_labels = save['train_labels_sanitized']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save # hint to help gc free up memory\n",
    "    print('Training_set',train_dataset.shape, train_labels.shape)\n",
    "    print('Validation_set',valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test_set',test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_set (192407, 784) (192407, 10)\n",
      "Validation_set (10000, 784) (10000, 10)\n",
      "Test_set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels):\n",
    "    \"\"\"重构数据集样式\"\"\"\n",
    "    dataset = dataset.reshape(dataset.shape[0], -1).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) \n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training_set',train_dataset.shape, train_labels.shape)\n",
    "print('Validation_set',valid_dataset.shape, valid_labels.shape)\n",
    "print('Test_set',test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"精确度\"\"\"\n",
    "#     correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1)) # bool\n",
    "#     print(correct_prediction)\n",
    "#     return tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # tf.cast:将bool转换成其他类型\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # mini-batch-size\n",
    "lam = 0.01 # 正则参数\n",
    "\n",
    "gragh = tf.Graph()\n",
    "with gragh.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) +\n",
    "        lam * tf.nn.l2_loss(weights)) # 增加 正则项\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 45.672272\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 11.4%\n",
      "Test accuracy: 11.5%\n",
      "Minibatch loss at step 200: 27.652506\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 54.4%\n",
      "Test accuracy: 59.9%\n",
      "Minibatch loss at step 400: 21.828186\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 64.9%\n",
      "Test accuracy: 71.6%\n",
      "Minibatch loss at step 600: 17.386768\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 69.2%\n",
      "Test accuracy: 76.3%\n",
      "Minibatch loss at step 800: 14.664577\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 71.4%\n",
      "Test accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 11.444107\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 72.8%\n",
      "Test accuracy: 80.4%\n",
      "Minibatch loss at step 1200: 9.809155\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.8%\n",
      "Test accuracy: 81.6%\n",
      "Minibatch loss at step 1400: 7.634736\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 75.0%\n",
      "Test accuracy: 82.7%\n",
      "Minibatch loss at step 1600: 6.642901\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.8%\n",
      "Test accuracy: 83.3%\n",
      "Minibatch loss at step 1800: 5.531523\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 76.8%\n",
      "Test accuracy: 84.1%\n",
      "Minibatch loss at step 2000: 4.473174\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.4%\n",
      "Test accuracy: 85.0%\n",
      "Minibatch loss at step 2200: 3.776019\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.0%\n",
      "Test accuracy: 85.7%\n",
      "Minibatch loss at step 2400: 3.445004\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.7%\n",
      "Test accuracy: 86.1%\n",
      "Minibatch loss at step 2600: 2.509677\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 86.8%\n",
      "Minibatch loss at step 2800: 2.131817\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 87.4%\n",
      "Minibatch loss at step 3000: 2.228035\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 87.7%\n",
      "Minibatch loss at step 3200: 1.806179\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.9%\n",
      "Minibatch loss at step 3400: 1.667016\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Test accuracy: 88.1%\n",
      "Minibatch loss at step 3600: 1.360944\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.7%\n",
      "Minibatch loss at step 3800: 1.252872\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.7%\n",
      "Minibatch loss at step 4000: 1.064282\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.8%\n",
      "Minibatch loss at step 4200: 1.047113\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.9%\n",
      "Minibatch loss at step 4400: 1.100934\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 89.2%\n",
      "Minibatch loss at step 4600: 1.135110\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.2%\n",
      "Minibatch loss at step 4800: 0.926805\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "with tf.Session(graph=gragh) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 200 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-hidden layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 5000\n",
    "hidden1_units = 1024\n",
    "n_samples = train_dataset.shape[0]\n",
    "\n",
    "with tf.name_scope('ANN') as scope:\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variable\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]))\n",
    "    \n",
    "    # hidden layer 1\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, num_labels],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) +\n",
    "        lam * tf.nn.l2_loss(weights1) + lam * tf.nn.l2_loss(weights2)) # 增加 正则项\n",
    "     \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False) # 保存全局训练步骤（global training step）的数值\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid = tf.nn.relu(tf.matmul(valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    # test\n",
    "    hidden_test = tf.nn.relu(tf.matmul(test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 4.978971\n",
      "Train accuracy: 75.4%\n",
      "Validation accuracy: 77.0%\n",
      "Test accuracy: 84.2%\n",
      "Loss at step 1: 4.616375\n",
      "Train accuracy: 78.1%\n",
      "Validation accuracy: 79.1%\n",
      "Test accuracy: 86.4%\n",
      "Loss at step 2: 4.408500\n",
      "Train accuracy: 79.1%\n",
      "Validation accuracy: 79.9%\n",
      "Test accuracy: 87.3%\n",
      "Loss at step 3: 4.242886\n",
      "Train accuracy: 79.8%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.9%\n",
      "Loss at step 4: 4.096069\n",
      "Train accuracy: 80.2%\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 88.3%\n",
      "Loss at step 5: 3.960530\n",
      "Train accuracy: 80.6%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.5%\n",
      "Loss at step 6: 3.833129\n",
      "Train accuracy: 81.0%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.6%\n",
      "Loss at step 7: 3.712298\n",
      "Train accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.8%\n",
      "Loss at step 8: 3.597161\n",
      "Train accuracy: 81.4%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 89.0%\n",
      "Loss at step 9: 3.487132\n",
      "Train accuracy: 81.6%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 10\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run() # 初始化变量\n",
    "    print(\"Initialized\")\n",
    "    for step in range(n_steps):\n",
    "        # dataset and labesl\n",
    "        for i in range(n_samples // batch_size):\n",
    "            # dataset and labesl\n",
    "            batch_data = train_dataset[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_labels = train_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "            # Run the computations. \n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        print(\"Loss at step %d: %f\" % (step, l))\n",
    "        print(\"Train accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_sets和valid的精确度比较平衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 8000\n",
    "hidden1_units = 1024\n",
    "\n",
    "with tf.name_scope('ANN') as scope:\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variable\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]))\n",
    "    \n",
    "    # hidden layer 1\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, num_labels],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "     \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False) # 保存全局训练步骤（global training step）的数值\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid = tf.nn.relu(tf.matmul(valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    # test\n",
    "    hidden_test = tf.nn.relu(tf.matmul(test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.299004\n",
      "Train accuracy: 13.5%\n",
      "Validation accuracy: 59.4%\n",
      "Test accuracy: 65.0%\n",
      "Loss at step 50: 0.528100\n",
      "Train accuracy: 85.0%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.7%\n",
      "Loss at step 100: 0.431688\n",
      "Train accuracy: 87.9%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 90.5%\n",
      "Loss at step 150: 0.358668\n",
      "Train accuracy: 90.0%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.6%\n",
      "Loss at step 200: 0.298744\n",
      "Train accuracy: 92.7%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.5%\n",
      "Loss at step 250: 0.245042\n",
      "Train accuracy: 94.6%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.6%\n",
      "Loss at step 300: 0.198047\n",
      "Train accuracy: 95.9%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.7%\n",
      "Loss at step 350: 0.162373\n",
      "Train accuracy: 97.3%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.5%\n",
      "Loss at step 400: 0.128728\n",
      "Train accuracy: 98.2%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.6%\n",
      "Loss at step 450: 0.102029\n",
      "Train accuracy: 98.9%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run() # 初始化变量\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # dataset and labesl\n",
    "        batch_data = train_dataset[:batch_size, :]\n",
    "        batch_labels = train_labels[:batch_size, :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        # Run the computations. \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 50 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据量少，导致train的精确度很高，但是验证集上的精确度却差很多，妥妥的过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 8000\n",
    "hidden1_units = 1024\n",
    "lam = 0.005\n",
    "\n",
    "with tf.name_scope('ANN') as scope:\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variable\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]))\n",
    "    \n",
    "    # hidden layer 1\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    # 增加dropout\n",
    "    keep_pro = tf.placeholder(tf.float32)\n",
    "    hidden1_drop = tf.nn.dropout(hidden1, keep_pro)\n",
    "    \n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, num_labels],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden1_drop, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "     \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False) # 保存全局训练步骤（global training step）的数值\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid = tf.nn.relu(tf.matmul(valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    # test\n",
    "    hidden_test = tf.nn.relu(tf.matmul(test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 0.719336\n",
      "Train accuracy: 79.8%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.6%\n",
      "Loss at step 10: 0.544086\n",
      "Train accuracy: 84.7%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 92.2%\n",
      "Loss at step 20: 0.489057\n",
      "Train accuracy: 86.2%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.2%\n",
      "Loss at step 30: 0.446530\n",
      "Train accuracy: 87.2%\n",
      "Validation accuracy: 87.9%\n",
      "Test accuracy: 93.9%\n",
      "Loss at step 40: 0.422733\n",
      "Train accuracy: 88.2%\n",
      "Validation accuracy: 88.4%\n",
      "Test accuracy: 94.4%\n",
      "Loss at step 50: 0.403037\n",
      "Train accuracy: 88.5%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.7%\n",
      "Loss at step 60: 0.379670\n",
      "Train accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 94.9%\n",
      "Loss at step 70: 0.376045\n",
      "Train accuracy: 89.4%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 95.0%\n",
      "Loss at step 80: 0.363014\n",
      "Train accuracy: 89.9%\n",
      "Validation accuracy: 89.4%\n",
      "Test accuracy: 95.2%\n",
      "Loss at step 90: 0.346940\n",
      "Train accuracy: 90.1%\n",
      "Validation accuracy: 89.6%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100 # 循环次数\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run() # 初始化变量\n",
    "    print(\"Initialized\")\n",
    "    for step in range(n_steps):\n",
    "        # dataset and labesl\n",
    "        for i in range(n_samples // batch_size):\n",
    "            # dataset and labesl\n",
    "            batch_data = train_dataset[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_labels = train_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:0.5} # 0.5\n",
    "            # Run the computations. \n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 10 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval({tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:1}), valid_labels)) # 1\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "                test_prediction.eval({tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:1}), test_labels)) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "lam = 0.002\n",
    "\n",
    "batch_size = 10000\n",
    "hidden1_units = 1024 # 第一层单元数\n",
    "hidden2_units = 512 # 第二层单元数\n",
    "\n",
    "with tf.name_scope('ANN') as scope:\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # 1 layer\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_units],\n",
    "                           stddev=1.0 / math.sqrt(float(image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_units]))\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    \n",
    "    # 2 layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden1_units))))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_units]))\n",
    "    \n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    \n",
    "    # 3 layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, num_labels],\n",
    "                           stddev=1.0 / math.sqrt(float(hidden2_units))))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # 增加dropout, 改善过拟合\n",
    "    keep_pro = tf.placeholder(tf.float32)\n",
    "    hidden2_drop = tf.nn.dropout(hidden2, keep_pro)\n",
    "    \n",
    "    # Train computation\n",
    "    logits = tf.matmul(hidden2_drop, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits) + \n",
    "        lam * tf.nn.l2_loss(weights1) +\n",
    "        lam * tf.nn.l2_loss(weights2) +\n",
    "        lam * tf.nn.l2_loss(weights3)) # 增加 正则项\n",
    "     \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.6, global_step, 1000, 0.98)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # validation\n",
    "    hidden_valid_1 = tf.nn.relu(tf.matmul(valid_dataset, weights1) + biases1)\n",
    "    hidden_valid_2 = tf.nn.relu(tf.matmul(hidden_valid_1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_2, weights3) + biases3)\n",
    "    \n",
    "    # test\n",
    "    hidden_test_1 = tf.nn.relu(tf.matmul(test_dataset, weights1) + biases1)\n",
    "    hidden_test_2 = tf.nn.relu(tf.matmul(hidden_test_1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.058116\n",
      "Train accuracy: 69.3%\n",
      "Validation accuracy: 67.7%\n",
      "Test accuracy: 73.2%\n",
      "Loss at step 10: 1.265482\n",
      "Train accuracy: 85.6%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.9%\n",
      "Loss at step 20: 0.969415\n",
      "Train accuracy: 87.2%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 93.1%\n",
      "Loss at step 30: 0.792380\n",
      "Train accuracy: 88.0%\n",
      "Validation accuracy: 87.7%\n",
      "Test accuracy: 93.9%\n",
      "Loss at step 40: 0.676623\n",
      "Train accuracy: 89.0%\n",
      "Validation accuracy: 88.1%\n",
      "Test accuracy: 94.0%\n",
      "Loss at step 50: 0.600525\n",
      "Train accuracy: 89.3%\n",
      "Validation accuracy: 88.4%\n",
      "Test accuracy: 94.4%\n",
      "Loss at step 60: 0.557666\n",
      "Train accuracy: 89.6%\n",
      "Validation accuracy: 88.5%\n",
      "Test accuracy: 94.3%\n",
      "Loss at step 70: 0.519929\n",
      "Train accuracy: 90.2%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.7%\n",
      "Loss at step 80: 0.546584\n",
      "Train accuracy: 88.7%\n",
      "Validation accuracy: 85.9%\n",
      "Test accuracy: 91.6%\n",
      "Loss at step 90: 0.482159\n",
      "Train accuracy: 91.0%\n",
      "Validation accuracy: 89.4%\n",
      "Test accuracy: 95.0%\n",
      "Loss at step 100: 0.482473\n",
      "Train accuracy: 90.7%\n",
      "Validation accuracy: 88.7%\n",
      "Test accuracy: 94.7%\n",
      "Loss at step 110: 0.466381\n",
      "Train accuracy: 91.2%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 95.0%\n",
      "Loss at step 120: 0.469977\n",
      "Train accuracy: 90.8%\n",
      "Validation accuracy: 89.3%\n",
      "Test accuracy: 94.9%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 200 # 循环次数\n",
    "n_samples = train_dataset.shape[0]\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run() # 初始化变量\n",
    "    print(\"Initialized\")\n",
    "    for step in range(n_steps):\n",
    "        for i in range(n_samples // batch_size):\n",
    "            # dataset and labesl\n",
    "            batch_data = train_dataset[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_labels = train_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:0.5} # 0.5\n",
    "            # Run the computations. \n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 10 == 0:\n",
    "            print(\"Loss at step %d: %f\" % (step, l))\n",
    "            print(\"Train accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval({tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:1}), valid_labels)) # 1\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "                test_prediction.eval({tf_train_dataset:batch_data, tf_train_labels:batch_labels, keep_pro:1}), test_labels)) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
