{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = \"./notMNIST.pickle\"\n",
    "\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "           / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 (16, 14, 14, 16)\n",
      "conv2 (16, 7, 7, 16)\n",
      "shape [16, 7, 7, 16]\n",
      "conv1 (10000, 14, 14, 16)\n",
      "conv2 (10000, 7, 7, 16)\n",
      "shape [10000, 7, 7, 16]\n",
      "conv1 (10000, 14, 14, 16)\n",
      "conv2 (10000, 7, 7, 16)\n",
      "shape [10000, 7, 7, 16]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5  # filter size\n",
    "depth = 16 # filter nums\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input Data\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels)\n",
    "    )\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    layer1_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)\n",
    "    )\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)\n",
    "    )\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)\n",
    "    )\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden, num_labels], stddev=0.1)\n",
    "    )\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1,2,2,1], padding=\"SAME\")\n",
    "        print(\"conv1\", conv.shape)\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1,2,2,1], padding=\"SAME\")\n",
    "        print(\"conv2\", conv.shape)\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        print(\"shape\",shape)\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training Computation\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.02).minimize(loss)\n",
    "    \n",
    "    # Prediction for train valid test\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n",
      "Minibatch loss at step 0: 3.673247\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 50: 1.780114\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 51.5%\n",
      "Minibatch loss at step 100: 1.502041\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.1%\n",
      "Minibatch loss at step 150: 0.684526\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 200: 0.890489\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 250: 1.024285\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 300: 0.665337\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 350: 0.310169\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 400: 1.099694\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 450: 0.843119\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 500: 1.868180\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 550: 0.337008\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 600: 0.595883\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 650: 0.283972\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 700: 1.024103\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 750: 1.106400\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 800: 0.430506\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 850: 0.336217\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 900: 0.854201\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 950: 1.256217\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1000: 0.812322\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1050: 0.969265\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1100: 0.912750\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1150: 0.970830\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1200: 0.729477\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1250: 1.268831\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1300: 0.591778\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1350: 1.070997\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1400: 0.338338\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1450: 0.775096\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 0.769346\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1550: 0.967657\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1600: 1.006246\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1650: 0.498288\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1700: 1.744627\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1750: 0.595902\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1800: 0.658087\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1850: 0.461236\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1900: 0.577713\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1950: 0.421029\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 0.570601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Test accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Init\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 50 == 0:\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape [16, 7, 7, 16]\n",
      "shape [10000, 7, 7, 16]\n",
      "shape [10000, 7, 7, 16]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5  # filter size\n",
    "depth = 16 # filter nums\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input Data\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels)\n",
    "    )\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    layer1_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)\n",
    "    )\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)\n",
    "    )\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)\n",
    "    )\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden, num_labels], stddev=0.1)\n",
    "    )\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    def model(data):\n",
    "        # conv layer 1\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding=\"SAME\")\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pool = tf.nn.max_pool(hidden,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding=\"VALID\")\n",
    "        # conv layer 2\n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1,1,1,1], padding=\"SAME\")\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pool = tf.nn.max_pool(hidden,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding=\"VALID\")\n",
    "        shape = pool.get_shape().as_list()\n",
    "        print(\"shape\",shape)\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training Computation\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.02).minimize(loss)\n",
    "    \n",
    "    # Prediction for train valid test\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n",
      "Minibatch loss at step 0: 3.646344\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 8.0%\n",
      "Minibatch loss at step 50: 1.871946\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 47.0%\n",
      "Minibatch loss at step 100: 1.435126\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 53.6%\n",
      "Minibatch loss at step 150: 0.584958\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 200: 0.783134\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 250: 1.232164\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 300: 0.846690\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 350: 0.373365\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 400: 1.160614\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 450: 0.898671\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 500: 1.738029\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 550: 0.233191\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 600: 0.617084\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 650: 0.273198\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 700: 1.076751\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 750: 1.175214\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 800: 0.274848\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 850: 0.295980\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 900: 0.715463\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 950: 0.991318\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 0.761625\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1050: 0.872029\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1100: 0.821196\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1150: 0.827475\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1200: 0.478837\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1250: 1.161493\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1300: 0.635115\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1350: 0.940645\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1400: 0.364621\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1450: 0.533653\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1500: 0.693474\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1550: 0.859217\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1600: 0.934593\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1650: 0.412839\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 1700: 1.877255\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1750: 0.683271\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1800: 0.630690\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1850: 0.495340\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 1900: 0.445015\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1950: 0.489576\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2000: 0.472206\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Test accuracy: 90.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Init\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 50 == 0:\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape [16, 7, 7, 16]\n",
      "shape [10000, 7, 7, 16]\n",
      "shape [10000, 7, 7, 16]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5  # filter size\n",
    "depth = 16 # filter nums\n",
    "num_hidden = 64\n",
    "\n",
    "initial_learning_rate = 0.05\n",
    "learning_rate_decay_factor = 0.95\n",
    "decay_steps = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input Data\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels)\n",
    "    )\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    # 增加dropout\n",
    "    tf_keep_pro = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    layer1_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)\n",
    "    )\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)\n",
    "    )\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    \n",
    "    layer3_weights = tf.Variable(\n",
    "        tf.truncated_normal([(image_size // 4) * (image_size // 4) * depth, num_hidden], stddev=0.1)\n",
    "    )\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    # Full Connected\n",
    "    layer4_weights = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden, num_classes], stddev=0.1)\n",
    "    )\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_classes]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    def model(data, keep_pro):\n",
    "        # conv layer 1\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding=\"SAME\")\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pool = tf.nn.max_pool(hidden,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding=\"VALID\")\n",
    "        # conv layer 2\n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1,1,1,1], padding=\"SAME\")\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pool = tf.nn.max_pool(hidden,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding=\"VALID\")\n",
    "        \n",
    "        # Full Connected\n",
    "        shape = pool.get_shape().as_list()\n",
    "        print(\"shape\",shape)\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # 增加Dropout\n",
    "        hidden_drop = tf.nn.dropout(hidden, keep_pro)\n",
    "    \n",
    "        return tf.matmul(hidden_drop, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training Computation\n",
    "    logits = model(tf_train_dataset, tf_keep_pro)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    # 增加learning_rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                               global_step,\n",
    "                                               decay_steps,\n",
    "                                               learning_rate_decay_factor,\n",
    "                                               staircase=True,\n",
    "                                               )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    # Prediction for train valid test\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, tf_keep_pro))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, tf_keep_pro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n",
      "Minibatch loss at step 0: 4.076749\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.301419\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 17.6%\n",
      "Minibatch loss at step 100: 2.298007\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 20.1%\n",
      "Minibatch loss at step 150: 1.481355\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 52.2%\n",
      "Minibatch loss at step 200: 1.480799\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 250: 1.303537\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 300: 1.181466\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 350: 0.864792\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 400: 1.259225\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 450: 0.944868\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 500: 1.899414\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 550: 0.545745\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 600: 0.778983\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 650: 0.753758\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 700: 0.899642\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 750: 1.778653\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 800: 0.764443\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 850: 0.461502\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 900: 1.363495\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 950: 1.242841\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 1.011023\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1050: 0.724679\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1100: 1.148757\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1150: 1.080786\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1200: 0.833594\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1250: 1.345386\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1300: 0.588492\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1350: 0.843549\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1400: 0.341096\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1450: 0.739008\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 0.857435\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1550: 1.131799\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1600: 1.365467\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1650: 0.603754\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1700: 1.547166\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1750: 1.055762\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1800: 0.497423\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1850: 0.497241\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1900: 0.808775\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1950: 0.790065\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 0.988449\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2050: 0.624038\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2100: 0.426872\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2150: 0.470905\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2200: 0.773698\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2250: 0.604654\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2300: 0.805079\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2350: 0.657161\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2400: 0.258516\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2450: 0.797958\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2500: 0.484425\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2550: 1.045257\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2600: 0.836618\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2650: 0.670906\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2700: 0.663814\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2750: 0.464320\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2800: 0.420296\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2850: 0.752561\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2900: 1.024324\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2950: 0.541700\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 0.767107\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 90.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Init\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset:batch_data, tf_train_labels:batch_labels, tf_keep_pro:0.5}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 50 == 0:\n",
    "            loss_list.append(l)\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(\n",
    "            {tf_train_dataset:batch_data, tf_train_labels:batch_labels, tf_keep_pro:1}), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(\n",
    "    {tf_train_dataset:batch_data, tf_train_labels:batch_labels, tf_keep_pro:1}), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16e84ca0208>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(0, 3050, 50), loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
